% Chapter 3: The Confidence System
% Formalizes confidence as epistemic commitment and its algebraic structure

\chapter{The Confidence System}
\label{ch:confidence}

\epigraph{%
  ``Probability is not about what is true. It is about what is reasonable to believe.''
}{E.T. Jaynes, \textit{Probability Theory: The Logic of Science}}

The confidence system is the algebraic foundation of CLAIR. This chapter defines
confidence formally, distinguishes it from probability, and develops the theory of
\emph{epistemic linearity}---treating evidence as a resource that cannot be
counted multiple times. We then establish the three monoid structures that govern
how confidence values combine, proving key properties in Lean~4 to connect
abstract theory to machine-verified implementation.

\section{Confidence as Epistemic Commitment}
\label{sec:conf-definition}

\subsection{The Problem with Probability}
\label{subsec:probability-problem}

Standard approaches to uncertain reasoning use probability. A probability measure
$P$ on a set $\Omega$ of outcomes satisfies the Kolmogorov axioms:
\begin{align}
  P(A) &\geq 0 & \text{(Non-negativity)} \\
  P(\Omega) &= 1 & \text{(Normalization)} \\
  P(A \cup B) &= P(A) + P(B) - P(A \cap B) & \text{(Additivity)}
\end{align}

For propositions, this implies the fundamental constraint:
\[
  P(\phi) + P(\lnot\phi) = 1
\]

This \emph{normalization requirement} creates three problems for modeling epistemic
states:

\paragraph{Balanced uncertainty.}
An agent confronting an unfamiliar topic may be uncertain about both $\phi$ and
$\lnot\phi$. If asked ``Is there intelligent life elsewhere in the universe?''
a reasonable response is low confidence in \emph{both} yes and no---not because
the evidence is balanced, but because there is insufficient evidence either way.
Probability forces $P(\text{yes}) + P(\text{no}) = 1$, conflating ``I don't know''
with ``the evidence is exactly balanced.''

\paragraph{Paraconsistent reasoning.}
Evidence sometimes supports both $\phi$ and $\lnot\phi$ before contradiction
resolution. A detective might have testimony that a suspect was present (supporting
guilt) and an alibi (supporting innocence), without yet knowing which is false.
Probability makes this impossible: $P(\phi) > 0.5$ and $P(\lnot\phi) > 0.5$ is
a contradiction.

\paragraph{Derivation semantics.}
In Bayesian reasoning, $P(A \land B) = P(A) \cdot P(B|A)$, where conditioning
captures the dependency structure. But for derivation---where $B$ follows from $A$
by some rule---there is no clear conditional to use. The semantics of ``$A$ is a
premise for $B$'' differs from ``$B$ is probable given $A$ is true.''

\subsection{Definition of Confidence}
\label{subsec:confidence-def}

CLAIR's confidence addresses these problems by dropping normalization:

\begin{definition}[Confidence]
\label{def:confidence}
A \emph{confidence value} is a real number $c \in [0,1]$. We write $\mathbf{C}$
for the set of confidence values:
\[
  \mathbf{C} = \{c \in \mathbb{R} \mid 0 \leq c \leq 1\}
\]
\end{definition}

The semantic interpretation differs from probability:

\begin{itemize}
  \item $c = 1$: Axiomatic acceptance (treated as foundational)
  \item $c = 0$: Complete rejection (treated as impossibility)
  \item $c = 0.5$: Maximal uncertainty (no evidence either direction)
  \item $c > 0.5$: Net evidence for acceptance
  \item $c < 0.5$: Net evidence for rejection
\end{itemize}

\begin{definition}[Epistemic commitment]
\label{def:epistemic-commitment}
Confidence represents \emph{epistemic commitment}: the degree to which an agent
commits to a proposition based on available evidence and reasoning. Unlike
probability:
\begin{enumerate}
  \item \textbf{No normalization}: $\conf(\phi) + \conf(\lnot\phi)$ need not equal 1.
  \item \textbf{Not frequency}: $\conf(\phi) = 0.9$ does not mean ``true 90\% of
        the time.''
  \item \textbf{Derivation-based}: Confidence propagates through inference rules,
        not conditioning.
\end{enumerate}
\end{definition}

\begin{example}[Non-normalized confidence]
Consider the proposition ``This code has no security vulnerabilities.'' An honest
assessment might be:
\begin{align*}
  \conf(\text{no vulnerabilities}) &= 0.4 \quad \text{(some evidence from testing)} \\
  \conf(\text{has vulnerabilities}) &= 0.3 \quad \text{(some evidence from complexity)}
\end{align*}
The sum $0.4 + 0.3 = 0.7 < 1$ reflects residual uncertainty---neither hypothesis
is well-supported. This is inexpressible in probability.
\end{example}

\subsection{Comparison with Subjective Logic}
\label{subsec:subjective-logic-comparison}

Jøsang's Subjective Logic~\citep{josang2016subjective} extends probability with
explicit uncertainty. An \emph{opinion} is a tuple $\omega = (b, d, u, a)$:
\begin{itemize}
  \item $b$: belief mass (evidence for)
  \item $d$: disbelief mass (evidence against)
  \item $u$: uncertainty mass (lack of evidence)
  \item $a$: base rate (prior probability)
\end{itemize}
with constraint $b + d + u = 1$.

CLAIR's confidence can be viewed as a simplification of Subjective Logic:
\[
  c = b + u \cdot a
\]
where we collapse uncertainty into the confidence value via the base rate. This
loses the $b/d/u$ decomposition but gains simplicity. The trade-off is appropriate
for CLAIR's focus on derivation tracking rather than uncertainty quantification.

\begin{remark}
CLAIR could be extended to full Subjective Logic opinions. The algebraic
structure developed in this chapter would generalize, with the three monoids
operating on opinion tuples. We leave this extension to future work.
\end{remark}

\section{Evidence as Affine Resource}
\label{sec:affine-evidence}

Confidence values quantify epistemic commitment, but they do not address a deeper
question: when can the \emph{same evidence} support multiple conclusions? This
section develops the theory of \emph{epistemic linearity}, which treats evidence
as a resource that cannot be counted multiple times.

\subsection{The Double-Counting Problem}
\label{subsec:double-counting}

Consider a witness who testifies to two distinct facts:
\begin{itemize}
  \item ``The suspect was at the scene'' (confidence $0.8$)
  \item ``The suspect was wearing a red jacket'' (confidence $0.8$)
\end{itemize}

If we aggregate the witness's testimony with itself, na\"ively applying the
$\oplus$ operation would give:
\[
  0.8 \oplus 0.8 = 0.8 + 0.8 - 0.64 = 0.96
\]
But this is \emph{double-counting}: the same testimony cannot provide
independent support. The correct aggregate should remain $0.8$.

\begin{definition}[Epistemic non-duplication]
\label{def:non-duplication}
A piece of evidence $e$ satisfies the \emph{non-duplication principle} if:
\[
  e \oplus e = e
\]
More generally, for any finite number of uses:
\[
  \underbrace{e \oplus e \oplus \cdots \oplus e}_{n \text{ times}} = e
\]
\end{definition}

This is \emph{idempotence} of evidence aggregation over identical sources.
Violating this principle creates a \emph{bootstrap vulnerability} where
repeated references to the same evidence amplify confidence spuriously.

\paragraph{Three manifestations of double-counting:}

1. \textbf{Self-introspection}: When a belief introspects itself, aggregating
   the result with the original overcounts. \Cref{ch:self-reference} shows
   how stratification prevents this at the type level.

2. \textbf{Correlated sources}: Two news articles citing the same anonymous
   source are not independent evidence. \Cref{ch:justification} develops
   $\delta$-parameterized aggregation to handle this.

3. \textbf{Shared premises}: In a justification DAG, multiple conclusions
   supported by the same premise should not each ``get full credit'' for it.

\subsection{Linear Logic Background}
\label{subsec:linear-logic}

Girard's linear logic~\citep{girard1987linear} provides the theoretical
foundation for resource-sensitive reasoning. The key insight is to reject the
\emph{contraction rule} of classical logic:

\[
  \frac{\Gamma, A, A \vdash B}{\Gamma, A \vdash B} \quad \text{(Contraction)}
\]

Contraction says: ``if we can prove $B$ using $A$ twice, we can prove $B$
using $A$ once (because we can duplicate $A$).'' Linear logic forbids this:
if you have one $A$, you can use it \emph{exactly once}.

\begin{table}[h]
\centering
\begin{tabular}{lllll}
\toprule
\textbf{Logic} & \textbf{Weakening} & \textbf{Contraction} & \textbf{Exchange} & \textbf{Use} \\
\midrule
Classical    & \checkmark & \checkmark & \checkmark & Any number \\
Affine       & \checkmark & $\times$      & \checkmark & At most once \\
Relevant     & $\times$    & \checkmark & \checkmark & At least once \\
Linear       & $\times$    & $\times$      & \checkmark & Exactly once \\
\bottomrule
\end{tabular}
\caption{Substructural logic hierarchy}
\label{tab:substructural}
\end{table}

For epistemic evidence, \emph{affine logic} is the appropriate discipline:
\begin{itemize}
  \item \textbf{No contraction}: Evidence cannot be used multiple times
  \item \textbf{Weakening allowed}: Not all evidence need be used
\end{itemize}

This matches epistemic practice: we can hold evidence without using it
(weakening), but we shouldn't count the same evidence twice (no contraction).

\subsection{Affine vs.\ Exponential Evidence}
\label{subsec:affine-vs-exponential}

Not all evidence should be affine. Some evidence genuinely supports multiple
conclusions:

\begin{example}[Axioms as exponential evidence]
The axiom $2 + 2 = 4$ can support ``$4 + 4 = 8$'' and ``$4 \times 2 = 8$''
simultaneously. This evidence should be marked \emph{exponential} (written $!e$)
to indicate it can be freely reused.
\end{example}

\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Affine Evidence} & \textbf{Exponential Evidence} \\
\midrule
Testimony from a witness & Mathematical axioms \\
Specific observation & Established definitions \\
Particular data point & Domain-general facts \\
Source-specific attack & Common knowledge \\
\bottomrule
\end{tabular}
\caption{Classification of evidence by reusability}
\label{tab:evidence-classification}
\end{table}

Linear logic marks exponential evidence with the \emph{of-course} operator
$!$. In CLAIR:
\[
  \begin{cases}
    e : \text{Evidence} & \text{can be used at most once (affine)} \\
    !e : \text{Evidence} & \text{can be used any number of times (exponential)}
  \end{cases}
\]

\subsection{Information-Theoretic Grounding}
\label{subsec:information-theoretic}

The non-duplication principle is not arbitrary---it follows from Shannon's
information theory~\citep{shannon1948mathematical}.

\begin{definition}[Mutual information]
\label{def:mutual-information}
For random variables $A$ and $B$, the mutual information is:
\[
  I(A; B) = H(A) + H(B) - H(A, B)
\]
where $H$ denotes Shannon entropy.
\end{definition}

When $A = B$ (perfect correlation):
\[
  I(A; A) = H(A) + H(A) - H(A) = H(A)
\]

\begin{theorem}[Information content of repeated evidence]
\label{thm:repeated-evidence-information}
Observing the same evidence twice provides no more information than observing
it once: $I(A; A, A) = I(A; A) = H(A)$.
\end{theorem}

\begin{proof}
By the chain rule of mutual information:
\[
  I(A; A, A) = I(A; A) + I(A; A|A) = H(A) + 0 = H(A)
\]
The conditional mutual information $I(A; A|A) = 0$ because $A$ provides no
additional information about itself when already known.
\end{proof}

Epistemic linearity is the type-theoretic encoding of this informational
principle: preventing double-counting at the syntactic level preserves the
correct information accounting at the semantic level.

\subsection{Type System for Affine Evidence}
\label{subsec:affine-type-system}

To enforce non-duplication statically, CLAIR uses a \emph{dual context} type
judgment:

\begin{definition}[Affine typing judgment]
\label{def:affine-judgment}
\[
  \Gamma; \Delta \vdash e : A @c \rightsquigarrow U
\]
means: in unrestricted context $\Gamma$ and affine context $\Delta$,
expression $e$ has type $A$ with confidence $c$, using affine evidence $U
\subseteq \Delta$.
\end{definition}

The key rule is aggregation with disjointness:

\begin{theorem}[Aggregation disjointness]
\label{thm:aggregation-disjointness}
Two beliefs can be aggregated only if their evidence sources are disjoint:
\[
  \frac{
    \Gamma; \Delta \vdash e_1 : \text{Belief}(A) @c_1 \rightsquigarrow U_1 \quad
    \Gamma; \Delta \vdash e_2 : \text{Belief}(A) @c_2 \rightsquigarrow U_2 \quad
    U_1 \cap U_2 = \emptyset
  }{
    \Gamma; \Delta \vdash \text{aggregate}(e_1, e_2) :
      \text{Belief}(A) @(c_1 \oplus c_2) \rightsquigarrow U_1 \cup U_2
  }
\]
\end{theorem}

The disjointness constraint $U_1 \cap U_2 = \emptyset$ prevents the same
evidence from being counted twice. If $e_1$ and $e_2$ share evidence, the
aggregation is ill-typed.

\begin{example}[Rejected aggregation]
\begin{lstlisting}[language=CLAIR]
let testimony : Evidence<SawEvent> = witness_says(...)

let deriv1 = derive EventHappened from testimony  -- Uses testimony
let deriv2 = derive SuspectPresent from testimony  -- Uses testimony

let combined = aggregate(deriv1, deriv2)  -- TYPE ERROR: testimony used twice
\end{lstlisting}
The type system rejects this at compile time.
\end{example}

\subsection{Connection to Correlation-Aware Aggregation}
\label{subsec:affine-correlation}

\Cref{sec:aggregation-monoid} introduced $\oplus$ as independent aggregation.
\Cref{ch:justification} will introduce $\oplus_\delta$ for correlated evidence.
Affine typing provides the limit case:

\begin{theorem}[Affine as $\delta=1$ enforcement]
\label{thm:affine-as-delta-one}
Aggregation under affine typing enforces $\delta = 1$ (perfect correlation)
for evidence with overlapping usage:
\[
  \text{aggregate}(e, e) \text{ ill-typed} \iff \oplus_{\delta=1}(c, c) = c
\]
\end{theorem}

Affine types provide \emph{static} prevention (compile-time) while
correlation-aware aggregation provides \emph{dynamic} handling (runtime).
Both enforce the same principle at different stages.

\subsection{Linearity and Defeat}
\label{subsec:linearity-defeat}

How does affine evidence interact with defeat operations
(\Cref{sec:defeat-operations})?

\begin{theorem}[Consumption irreversibility]
\label{thm:consumption-irreversibility}
Once affine evidence $e$ is consumed in a derivation, defeat of that
derivation does not return $e$ to the affine context.
\end{theorem}

\begin{proof}
The typing rules only move evidence from $\Delta$ to usage sets $U$.
No rule adds evidence back to $\Delta$. Defeat rules transform confidence
but do not modify contexts. Therefore evidence consumption is monotonic
and irreversible. \qed
\end{proof}

This has important implications:

\paragraph{Partial undercut}: If evidence $e$ supports $P$ with confidence $c$,
and this support is undercut by strength $d = 0.5$, the resulting confidence
is $c \times 0.5$. The evidence $e$ remains consumed---partial undercut does
\emph{not} release ``half'' of $e$ for other uses.

\paragraph{Defeat evidence is also affine}: The evidence used for undercut or
rebut follows the same discipline:
\[
  \frac{
    \Gamma; \Delta_1 \vdash D : \text{Belief}(A) @c_D \rightsquigarrow U_D \quad
    \Gamma; \Delta_2 \vdash d : \text{Undercuts}(D) @c_d \rightsquigarrow U_d \quad
    U_D \cap U_d = \emptyset
  }{
    \Gamma; \Delta_1 \cup \Delta_2 \vdash \text{undercut}(D, d) :
      \text{Belief}(A) @(c_D \times (1 - c_d)) \rightsquigarrow U_D \cup U_d
  }
\]

\paragraph{Source-level defeaters are exponential}: Evidence that attacks a
\emph{source's reliability} (rather than a specific inference) should be marked
exponential, as it affects all derivations from that source:

\begin{example}[Exponential defeater]
\begin{lstlisting}[language=CLAIR]
let testimony1 = witness_says("suspect was at scene")
let testimony2 = witness_says("suspect wore red jacket")
let drunk : !Evidence<WitnessWasDrunk> = blood_test(...)  -- Exponential

let undercut1 = undercut(testimony1, drunk)  -- OK: drunk not consumed
let undercut2 = undercut(testimony2, drunk)  -- OK: drunk still available
\end{lstlisting}
\end{example}

\subsection{Exponential Promotion}
\label{subsec:exponential-promotion}

When can evidence be promoted from affine to exponential?

\begin{definition}[Exponential promotion]
\label{def:promotion}
If a derivation uses only unrestricted (exponential) evidence, its result
can be marked exponential:
\[
  \frac{
    \Gamma; \cdot \vdash e : A @c \rightsquigarrow \emptyset
  }{
    \Gamma; \Delta \vdash \text{promote}(e) : !A @c \rightsquigarrow \emptyset
  }
\]
\end{definition}

This allows ``evidence that didn't depend on affine resources'' to be freely
shared. Examples include:
\begin{itemize}
  \item Mathematical proofs (only axioms, no empirical data)
  \item Logical tautologies
  \item Analytic definitions
\end{itemize}

\subsection{Decidability of Affine Type Checking}
\label{subsec:affine-decidability}

A critical question: is type checking with affine evidence decidable?

\begin{theorem}[Decidability of affine type checking]
\label{thm:affine-decidability}
Type checking for CLAIR with affine evidence is decidable in polynomial time:
$O(n^2)$ where $n$ is the size of the expression.
\end{theorem}

\begin{proof}[Proof sketch]
By structural induction on the expression $e$:
\begin{enumerate}
  \item \textbf{Variables}: Lookup in $\Gamma$ or $\Delta$ is $O(n)$.
  \item \textbf{Application}: Recursively type check subexpressions, then verify
        $U_1 \cap U_2 = \emptyset$. Finite set intersection is $O(n)$.
  \item \textbf{Aggregation}: Same as application.
  \item \textbf{Lambdas}: Extend context and recurse.
\end{enumerate}
All operations are structural with finite context and set operations.
No unbounded search is required. \qed
\end{proof}

The algorithm uses \emph{bidirectional type checking} with usage tracking:
\begin{itemize}
  \item \textbf{Synthesis mode}: Given $\Gamma; \Delta$ and $e$, compute $A$, $c$, and $U$.
  \item \textbf{Checking mode}: Given $\Gamma; \Delta$, $e$, and claimed $A$, $c$, verify.
\end{itemize}

Usage sets propagate bottom-up, ensuring disjointness is checked locally at
each aggregation point.

\begin{remark}[Distinction from CPL decidability]
This decidability result is for \emph{type checking}, not validity checking in
Confidence-Bounded Provability Logic (CPL). CPL validity is likely undecidable
(\Cref{ch:self-reference}), but type checking---verifying that a well-typed
derivation exists---is decidable because it involves only finite structural
analysis, not quantification over all models.
\end{remark}

\subsection{Summary: The Affine Discipline}
\label{subsec:affine-summary}

The affine evidence system provides:

\begin{enumerate}
  \item \textbf{Static guarantees}: The type system prevents double-counting at
        compile time.

  \item \textbf{Information-theoretic grounding}: Non-duplication follows from
        Shannon's mutual information theory.

  \item \textbf{Flexibility}: The exponential $!$ marker allows genuinely
        reusable evidence (axioms, definitions).

  \item \textbf{Decidability}: Type checking remains polynomial-time.

  \item \textbf{Integration}: Affine typing complements stratification
        (\Cref{ch:self-reference}) and correlation-aware aggregation
        (\Cref{ch:justification}) to form a coherent system.
\end{enumerate}

Table~\ref{tab:affine-interactions} summarizes how affine evidence interacts
with other CLAIR mechanisms.

\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Mechanism} & \textbf{Prevents} \\
\midrule
Affine typing & All evidence duplication at type level \\
Correlation-aware aggregation ($\oplus_\delta$) & Runtime-discovered correlation \\
Stratification & Paradoxical self-reference \\
Löb discount ($g(c) = c^2$) & Bootstrapping via self-soundness \\
\bottomrule
\end{tabular}
\caption{Multi-layer protection against epistemic errors}
\label{tab:affine-interactions}
\end{table}

% ============================================================================
% END OF NEW SECTION
% ============================================================================

\section{The Multiplication Monoid}
\label{sec:multiplication-monoid}

When a conclusion is derived from premises, its confidence depends on the
premises' confidences. The simplest case is conjunctive derivation: both
premises must hold for the conclusion to follow.

\subsection{Conjunctive Confidence Propagation}
\label{subsec:conjunctive-propagation}

\begin{definition}[Confidence multiplication]
\label{def:conf-multiplication}
For confidence values $a, b \in \mathbf{C}$, their \emph{multiplicative combination}
is standard multiplication:
\[
  a \cdot b = a \times b
\]
\end{definition}

This models the intuition that deriving $C$ from $A$ and $B$ requires both to
be true. If we are 90\% confident in $A$ and 80\% confident in $B$, our
confidence in $C$ (derived from both) is at most $0.9 \times 0.8 = 0.72$.

\begin{theorem}[Multiplication preserves bounds]
\label{thm:mul-bounded}
For all $a, b \in \mathbf{C}$:
\[
  a \cdot b \in \mathbf{C}
\]
\end{theorem}

\begin{proof}
We prove both bounds:
\begin{enumerate}
  \item $a \cdot b \geq 0$: Since $a \geq 0$ and $b \geq 0$, their product is
        non-negative.
  \item $a \cdot b \leq 1$: Since $b \leq 1$, we have $a \cdot b \leq a \cdot 1 = a \leq 1$.
\end{enumerate}
\end{proof}

\begin{theorem}[Multiplication monoid]
\label{thm:mul-monoid}
$(\mathbf{C}, \cdot, 1)$ is a commutative monoid with absorbing element $0$:
\begin{enumerate}
  \item Associativity: $(a \cdot b) \cdot c = a \cdot (b \cdot c)$
  \item Commutativity: $a \cdot b = b \cdot a$
  \item Identity: $1 \cdot a = a \cdot 1 = a$
  \item Absorption: $0 \cdot a = a \cdot 0 = 0$
\end{enumerate}
\end{theorem}

\begin{proof}
All properties follow from standard real number arithmetic on $[0,1]$.
\end{proof}

\subsection{The Derivation Monotonicity Principle}
\label{subsec:derivation-monotonicity}

A fundamental property of CLAIR is that derivation can only decrease confidence:

\begin{theorem}[Derivation monotonicity]
\label{thm:derivation-monotonicity}
For all $a, b \in \mathbf{C}$:
\[
  a \cdot b \leq \min(a, b)
\]
In particular, $a \cdot b \leq a$ and $a \cdot b \leq b$.
\end{theorem}

\begin{proof}
Since $b \leq 1$, we have $a \cdot b \leq a \cdot 1 = a$.
By commutativity, $a \cdot b \leq b$.
Therefore $a \cdot b \leq \min(a, b)$.
\end{proof}

\begin{corollary}[No confidence amplification]
No sequence of conjunctive derivations can increase confidence. If $c_0$ is
the confidence of a foundational belief and $c_n$ is derived through $n$
multiplicative steps, then $c_n \leq c_0$.
\end{corollary}

This principle is essential for CLAIR's epistemology: derived beliefs are
never more confident than their sources. Certainty ($c = 1$) is reserved for
axioms, not conclusions.

\subsection{Connection to T-norms}
\label{subsec:tnorms}

Confidence multiplication is an instance of a \emph{t-norm} from fuzzy logic.
A t-norm $T : [0,1]^2 \to [0,1]$ is a binary operation satisfying:
\begin{enumerate}
  \item Commutativity: $T(a, b) = T(b, a)$
  \item Associativity: $T(T(a, b), c) = T(a, T(b, c))$
  \item Monotonicity: $a \leq a' \Rightarrow T(a, b) \leq T(a', b)$
  \item Identity: $T(a, 1) = a$
\end{enumerate}

The standard t-norms are:
\begin{itemize}
  \item \textbf{Product} (Algebraic): $T_P(a, b) = a \cdot b$
  \item \textbf{G\"odel} (Minimum): $T_G(a, b) = \min(a, b)$
  \item \textbf{\L ukasiewicz} (Bounded): $T_L(a, b) = \max(0, a + b - 1)$
\end{itemize}

CLAIR uses the product t-norm for conjunctive derivation. The choice is
motivated by its ``survival probability'' interpretation: if $a$ and $b$
are independent probabilities of ``success,'' then $a \cdot b$ is the
probability both succeed.

\begin{remark}
The G\"odel t-norm (minimum) provides an alternative that preserves more
confidence---see \S\ref{sec:minimum-monoid}. The choice between them
is semantic, not algebraic.
\end{remark}

\section{The Minimum Monoid}
\label{sec:minimum-monoid}

Sometimes a conservative estimate is more appropriate than multiplicative
combination. The minimum operation captures ``confidence limited by the
weakest link.''

\subsection{Conservative Combination}
\label{subsec:conservative-combination}

\begin{definition}[Minimum combination]
\label{def:minimum}
For confidence values $a, b \in \mathbf{C}$:
\[
  \min(a, b) = \begin{cases} a & \text{if } a \leq b \\ b & \text{otherwise} \end{cases}
\]
\end{definition}

\begin{theorem}[Minimum preserves bounds]
\label{thm:min-bounded}
For all $a, b \in \mathbf{C}$:
\[
  \min(a, b) \in \mathbf{C}
\]
\end{theorem}

\begin{proof}
$\min(a, b)$ is either $a$ or $b$, both in $[0,1]$.
\end{proof}

\begin{theorem}[Minimum semilattice]
\label{thm:min-semilattice}
$(\mathbf{C}, \min, 1)$ is a bounded meet-semilattice:
\begin{enumerate}
  \item Associativity: $\min(\min(a, b), c) = \min(a, \min(b, c))$
  \item Commutativity: $\min(a, b) = \min(b, a)$
  \item Idempotence: $\min(a, a) = a$
  \item Identity: $\min(1, a) = a$
\end{enumerate}
\end{theorem}

\begin{proof}
Standard properties of the minimum operation on ordered sets.
\end{proof}

\subsection{Comparison with Multiplication}
\label{subsec:min-vs-mul}

An important relationship between the two operations:

\begin{theorem}[Minimum dominates multiplication]
\label{thm:min-ge-mul}
For all $a, b \in \mathbf{C}$:
\[
  \min(a, b) \geq a \cdot b
\]
\end{theorem}

\begin{proof}
Without loss of generality, assume $a \leq b$. Then $\min(a, b) = a$.
Since $b \leq 1$, we have $a \cdot b \leq a \cdot 1 = a = \min(a, b)$.
\end{proof}

This means minimum is \emph{more optimistic} than multiplication: it preserves
more confidence. The semantic difference:
\begin{itemize}
  \item \textbf{Multiplication}: ``Both premises independently needed; compound
        failure modes.''
  \item \textbf{Minimum}: ``Conclusion limited by weakest link; no additional
        failure from combination.''
\end{itemize}

\begin{example}[When to use each]
Consider deriving $C$ from premises $A$ (confidence 0.9) and $B$ (confidence 0.8):
\begin{itemize}
  \item If $A$ and $B$ are \emph{independent} conditions for $C$, use
        multiplication: $\conf(C) = 0.9 \times 0.8 = 0.72$.
  \item If $B$ is a \emph{weaker version} of $A$, use minimum:
        $\conf(C) = \min(0.9, 0.8) = 0.8$.
\end{itemize}
\end{example}

\section{The Aggregation Monoid}
\label{sec:aggregation-monoid}

When multiple independent pieces of evidence support the same conclusion,
confidence should \emph{increase}. This requires a different operation.

\subsection{Probabilistic OR}
\label{subsec:oplus-definition}

\begin{definition}[Probabilistic OR / Oplus]
\label{def:oplus}
For confidence values $a, b \in \mathbf{C}$, their \emph{aggregation} is:
\[
  a \oplus b = a + b - a \cdot b
\]
\end{definition}

The formula has several equivalent forms:
\begin{align}
  a \oplus b &= a + b(1 - a) \label{eq:oplus-form1} \\
  a \oplus b &= a(1 - b) + b \label{eq:oplus-form2} \\
  a \oplus b &= 1 - (1 - a)(1 - b) \label{eq:oplus-demorgan}
\end{align}

The last form (\ref{eq:oplus-demorgan}) reveals the De Morgan duality with
multiplication: $a \oplus b$ is the complement of the product of complements.

\begin{theorem}[Oplus preserves bounds]
\label{thm:oplus-bounded}
For all $a, b \in \mathbf{C}$:
\[
  a \oplus b \in \mathbf{C}
\]
\end{theorem}

\begin{proof}
\textbf{Lower bound}: Using form (\ref{eq:oplus-form1}):
\[
  a \oplus b = a + b(1 - a)
\]
Since $a \geq 0$, $b \geq 0$, and $(1 - a) \geq 0$ (because $a \leq 1$),
we have $b(1 - a) \geq 0$, thus $a \oplus b \geq 0$.

\textbf{Upper bound}: Using form (\ref{eq:oplus-form1}) again:
\[
  a \oplus b = a + b(1 - a)
\]
Since $b \leq 1$ and $(1 - a) \leq 1$, we have $b(1 - a) \leq 1 - a$.
Therefore $a \oplus b \leq a + (1 - a) = 1$.
\end{proof}

\subsection{Aggregation Monoid Structure}
\label{subsec:oplus-monoid}

\begin{theorem}[Oplus monoid]
\label{thm:oplus-monoid}
$(\mathbf{C}, \oplus, 0)$ is a commutative monoid with absorbing element $1$:
\begin{enumerate}
  \item Associativity: $(a \oplus b) \oplus c = a \oplus (b \oplus c)$
  \item Commutativity: $a \oplus b = b \oplus a$
  \item Identity: $0 \oplus a = a \oplus 0 = a$
  \item Absorption: $1 \oplus a = a \oplus 1 = 1$
\end{enumerate}
\end{theorem}

\begin{proof}
\textbf{Commutativity}: $a \oplus b = a + b - ab = b + a - ba = b \oplus a$.

\textbf{Associativity}:
\begin{align*}
  (a \oplus b) \oplus c &= (a + b - ab) + c - (a + b - ab)c \\
  &= a + b + c - ab - ac - bc + abc
\end{align*}
\begin{align*}
  a \oplus (b \oplus c) &= a + (b + c - bc) - a(b + c - bc) \\
  &= a + b + c - bc - ab - ac + abc
\end{align*}
Both expressions equal $a + b + c - ab - ac - bc + abc$.

\textbf{Identity}: $0 \oplus a = 0 + a - 0 \cdot a = a$.

\textbf{Absorption}: $1 \oplus a = 1 + a - 1 \cdot a = 1$.
\end{proof}

\subsection{Confidence-Increasing Property}
\label{subsec:confidence-increasing}

Unlike multiplication, $\oplus$ \emph{increases} confidence:

\begin{theorem}[Oplus is confidence-increasing]
\label{thm:oplus-increasing}
For all $a, b \in \mathbf{C}$:
\[
  a \oplus b \geq \max(a, b)
\]
\end{theorem}

\begin{proof}
Using form (\ref{eq:oplus-form1}): $a \oplus b = a + b(1-a)$.
Since $b(1-a) \geq 0$, we have $a \oplus b \geq a$.
By commutativity, $a \oplus b \geq b$.
Therefore $a \oplus b \geq \max(a, b)$.
\end{proof}

\begin{corollary}[Diminishing returns]
\label{cor:diminishing-returns}
The marginal gain from additional evidence decreases as confidence grows:
\[
  \frac{\partial}{\partial b}(a \oplus b) = 1 - a
\]
When $a$ is already high, additional evidence contributes less.
\end{corollary}

\begin{example}[Aggregation of weak evidence]
Suppose we have ten independent pieces of weak evidence, each with confidence
$0.3$. The combined confidence is:
\[
  \underbrace{0.3 \oplus 0.3 \oplus \cdots \oplus 0.3}_{10 \text{ times}}
  = 1 - (1 - 0.3)^{10} = 1 - 0.7^{10} \approx 0.972
\]
Ten weak independent witnesses produce high combined confidence.
\end{example}

\subsection{The ``Survival of Doubt'' Interpretation}
\label{subsec:survival-of-doubt}

The formula $a \oplus b = 1 - (1-a)(1-b)$ admits a probability-theoretic
interpretation:
\begin{itemize}
  \item $(1 - a)$ is the ``doubt'' in evidence $a$
  \item $(1 - a)(1 - b)$ is the probability \emph{both} pieces of evidence
        fail (assuming independence)
  \item $a \oplus b$ is the probability \emph{at least one} succeeds
\end{itemize}

This ``survival of doubt'' interpretation explains why aggregation increases
confidence: more independent evidence means more chances for at least one
to be correct.

\section{Non-Semiring Structure}
\label{sec:non-semiring}

A natural question: do $\oplus$ and $\cdot$ form a semiring? The answer is no.

\subsection{Distributivity Failure}
\label{subsec:distributivity-failure}

\begin{theorem}[Non-distributivity]
\label{thm:non-distributivity}
The operations $\oplus$ and $\cdot$ do \emph{not} satisfy distributivity:
\[
  a \cdot (b \oplus c) \neq (a \cdot b) \oplus (a \cdot c)
\]
in general.
\end{theorem}

\begin{proof}
By counterexample. Let $a = b = c = 0.5$:
\begin{align*}
  a \cdot (b \oplus c) &= 0.5 \cdot (0.5 + 0.5 - 0.25) = 0.5 \cdot 0.75 = 0.375 \\
  (a \cdot b) \oplus (a \cdot c) &= 0.25 + 0.25 - 0.25 \cdot 0.25 = 0.5 - 0.0625 = 0.4375
\end{align*}
Since $0.375 \neq 0.4375$, distributivity fails.
\end{proof}

\subsection{Implications for CLAIR}
\label{subsec:non-semiring-implications}

The failure of distributivity means:

\begin{enumerate}
  \item \textbf{Operations are context-dependent}: The choice between $\cdot$,
        $\min$, and $\oplus$ depends on the justification structure, not
        algebraic laws.

  \item \textbf{Order matters}: In expressions mixing $\cdot$ and $\oplus$,
        parentheses are semantically significant.

  \item \textbf{No ring-theoretic tools}: We cannot apply ring homomorphisms
        or ideals to confidence algebra.
\end{enumerate}

This is not a limitation but a \emph{feature}: the algebra correctly models
that aggregation and derivation are fundamentally different operations that
do not interact algebraically.

\section{Defeat Operations}
\label{sec:defeat-operations}

Beyond derivation and aggregation, CLAIR requires operations for \emph{defeat}:
when evidence undermines a belief.

\subsection{Undercut: Attacking the Inference}
\label{subsec:undercut}

Following Pollock~\citep{pollock1987defeasible}, an \emph{undercutting defeater}
attacks the inferential link, not the conclusion directly.

\begin{definition}[Undercut]
\label{def:undercut}
For confidence $c$ in a conclusion and defeat strength $d$:
\[
  \undercut(c, d) = c \cdot (1 - d)
\]
\end{definition}

\begin{example}[Undercutting defeat]
Consider the inference: ``The object looks red, therefore it is red.''
An undercut ``The room has red lighting'' doesn't claim the object isn't red;
it undermines the inference from appearance to reality.

If $\conf(\text{looks red} \Rightarrow \text{is red}) = 0.9$ and
$\conf(\text{red lighting}) = 0.6$, then:
\[
  \undercut(0.9, 0.6) = 0.9 \cdot (1 - 0.6) = 0.9 \cdot 0.4 = 0.36
\]
The inference confidence drops from 0.9 to 0.36.
\end{example}

\begin{theorem}[Undercut preserves bounds]
\label{thm:undercut-bounded}
For all $c, d \in \mathbf{C}$:
\[
  \undercut(c, d) \in \mathbf{C}
\]
\end{theorem}

\begin{proof}
Since $d \leq 1$, we have $(1 - d) \geq 0$.
Since $c \geq 0$, we have $c \cdot (1 - d) \geq 0$.
Since $c \leq 1$ and $(1 - d) \leq 1$, we have $c \cdot (1 - d) \leq 1$.
\end{proof}

\begin{theorem}[Undercut composition]
\label{thm:undercut-composition}
Sequential undercuts compose via $\oplus$:
\[
  \undercut(\undercut(c, d_1), d_2) = \undercut(c, d_1 \oplus d_2)
\]
\end{theorem}

\begin{proof}
\begin{align*}
  \undercut(\undercut(c, d_1), d_2) &= c(1 - d_1)(1 - d_2) \\
  &= c(1 - d_1 - d_2 + d_1 d_2) \\
  &= c(1 - (d_1 + d_2 - d_1 d_2)) \\
  &= c(1 - (d_1 \oplus d_2)) \\
  &= \undercut(c, d_1 \oplus d_2)
\end{align*}
\end{proof}

This beautiful result shows that defeat strengths aggregate via $\oplus$: multiple
undercuts combine as if their doubts aggregated.

\subsection{Rebut: Competing Evidence}
\label{subsec:rebut}

A \emph{rebutting defeater} provides counter-evidence for the conclusion directly.

\begin{definition}[Rebut]
\label{def:rebut}
For confidence $c_{\text{for}}$ in favor and $c_{\text{against}}$ against:
\[
  \rebut(c_{\text{for}}, c_{\text{against}}) =
  \begin{cases}
    \displaystyle\frac{c_{\text{for}}}{c_{\text{for}} + c_{\text{against}}} & \text{if } c_{\text{for}} + c_{\text{against}} > 0 \\[2ex]
    0.5 & \text{if } c_{\text{for}} = c_{\text{against}} = 0
  \end{cases}
\]
\end{definition}

The formula treats evidence symmetrically: the resulting confidence is the
``market share'' of supporting evidence.

\begin{theorem}[Rebut preserves bounds]
\label{thm:rebut-bounded}
For all $c_{\text{for}}, c_{\text{against}} \in \mathbf{C}$:
\[
  \rebut(c_{\text{for}}, c_{\text{against}}) \in \mathbf{C}
\]
\end{theorem}

\begin{proof}
If $c_{\text{for}} + c_{\text{against}} = 0$, the result is $0.5 \in [0,1]$.
Otherwise:
\begin{itemize}
  \item $\rebut \geq 0$ because $c_{\text{for}} \geq 0$ and the denominator is positive.
  \item $\rebut \leq 1$ because $c_{\text{for}} \leq c_{\text{for}} + c_{\text{against}}$.
\end{itemize}
\end{proof}

\begin{theorem}[Rebut antisymmetry]
\label{thm:rebut-antisymmetry}
\[
  \rebut(a, b) + \rebut(b, a) = 1
\]
\end{theorem}

\begin{proof}
When $a + b > 0$:
\[
  \rebut(a, b) + \rebut(b, a) = \frac{a}{a+b} + \frac{b}{a+b} = \frac{a+b}{a+b} = 1
\]
When $a = b = 0$: $\rebut(a,b) = \rebut(b,a) = 0.5$, so the sum is $1$.
\end{proof}

\subsection{Semantic Difference: Undercut vs.\ Rebut}
\label{subsec:undercut-vs-rebut}

The two defeat mechanisms serve different roles:

\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Aspect} & \textbf{Undercut} & \textbf{Rebut} \\
\midrule
Target & Inference link & Conclusion \\
Effect & Multiplicative discount & Proportional competition \\
Formula & $c \cdot (1 - d)$ & $c_{\text{for}} / (c_{\text{for}} + c_{\text{against}})$ \\
Composition & Via $\oplus$ & Via weighted average \\
\bottomrule
\end{tabular}
\end{center}

Both are essential for modeling defeasible reasoning in the justification DAG
(see \Cref{ch:justification}).

\section{Lean 4 Formalization}
\label{sec:lean-formalization}

The confidence algebra is formalized in Lean~4 using Mathlib's \texttt{unitInterval}
type.

\subsection{Type Definition}
\label{subsec:lean-type}

Mathlib defines:
\begin{lstlisting}[language=Lean]
abbrev unitInterval : Set ℝ := Set.Icc 0 1
notation "I" => unitInterval
\end{lstlisting}

CLAIR uses this directly:
\begin{lstlisting}[language=Lean]
abbrev Confidence := unitInterval
\end{lstlisting}

This provides:
\begin{itemize}
  \item Multiplication as a closed operation (via \texttt{LinearOrderedCommMonoidWithZero})
  \item The \texttt{symm} operation $x \mapsto 1 - x$ with full properties
  \item Bound lemmas (\texttt{nonneg}, \texttt{le\_one})
  \item The \texttt{unit\_interval} tactic for automating proofs
\end{itemize}

\subsection{Oplus Definition and Proofs}
\label{subsec:lean-oplus}

\begin{lstlisting}[language=Lean]
def oplus (a b : Confidence) : Confidence :=
  ⟨(a : ℝ) + (b : ℝ) - (a : ℝ) * (b : ℝ), by
    constructor
    · -- Lower bound: 0 ≤ a + b - ab
      have h1 : 0 ≤ 1 - (a : ℝ) := a.one_minus_nonneg
      have h2 : 0 ≤ (b : ℝ) * (1 - (a : ℝ)) := mul_nonneg b.nonneg h1
      linarith [a.nonneg]
    · -- Upper bound: a + b - ab ≤ 1
      have h1 : (b : ℝ) * (1 - (a : ℝ)) ≤ 1 - (a : ℝ) := by
        apply mul_le_of_le_one_left a.one_minus_nonneg b.le_one
      linarith [a.le_one]⟩
\end{lstlisting}

Key theorems are proven using the \texttt{ring} tactic:
\begin{lstlisting}[language=Lean]
theorem oplus_comm (a b : Confidence) : a ⊕ b = b ⊕ a := by
  apply Subtype.ext; simp only [oplus]; ring

theorem oplus_assoc (a b c : Confidence) : (a ⊕ b) ⊕ c = a ⊕ (b ⊕ c) := by
  apply Subtype.ext; simp only [oplus]; ring
\end{lstlisting}

\subsection{Undercut via Symm}
\label{subsec:lean-undercut}

Undercut leverages Mathlib's \texttt{symm}:
\begin{lstlisting}[language=Lean]
def undercut (c d : Confidence) : Confidence := c * symm d
\end{lstlisting}

The composition theorem:
\begin{lstlisting}[language=Lean]
theorem undercut_compose (c d₁ d₂ : Confidence) :
    undercut (undercut c d₁) d₂ = undercut c (d₁ ⊕ d₂) := by
  apply Subtype.ext; simp [undercut, oplus, symm]; ring
\end{lstlisting}

\subsection{Verification Summary}
\label{subsec:verification-summary}

The Lean formalization verifies:

\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Operation} & \textbf{Property} & \textbf{Status} \\
\midrule
Multiplication & Bounded & $\checkmark$ (Mathlib) \\
Multiplication & Commutative monoid & $\checkmark$ (Mathlib) \\
Minimum & Bounded & $\checkmark$ \\
Minimum & Semilattice & $\checkmark$ \\
Oplus & Bounded & $\checkmark$ \\
Oplus & Commutative monoid & $\checkmark$ \\
Oplus & Confidence-increasing & $\checkmark$ \\
Undercut & Bounded & $\checkmark$ \\
Undercut & Composition via $\oplus$ & $\checkmark$ \\
Rebut & Bounded & $\checkmark$ \\
Rebut & Antisymmetry & $\checkmark$ \\
\bottomrule
\end{tabular}
\end{center}

Total: approximately 500 lines of Lean code including proofs.

\section{Conclusion}
\label{sec:confidence-conclusion}

This chapter established the algebraic and type-theoretic foundation of CLAIR:

\begin{enumerate}
  \item \textbf{Confidence is not probability}: It represents epistemic commitment
        without normalization, enabling paraconsistent reasoning.

  \item \textbf{Evidence is an affine resource}: Linear logic provides the
        theoretical foundation for treating evidence as non-duplicable. The
        dual-context type system $\Gamma; \Delta \vdash e : A @c \rightsquigarrow U$
        prevents double-counting at compile time, grounded in Shannon's
        information theory.

  \item \textbf{Three monoids, not a semiring}: Multiplication (derivation),
        minimum (conservative), and oplus (aggregation) serve distinct semantic
        roles and do not distribute.

  \item \textbf{Defeat operations}: Undercut and rebut formalize how evidence
        can undermine beliefs, with undercuts composing beautifully via oplus.
        Affine typing ensures defeat evidence follows the same discipline.

  \item \textbf{Machine-verified}: The algebra is formalized in Lean~4, ensuring
        type safety and algebraic correctness. Affine type checking is decidable
        in polynomial time.
\end{enumerate}

The confidence system provides the numeric and type-theoretic foundation. The next
chapter develops the \emph{structural} foundation: how beliefs connect through
justification DAGs.

%% ============================================================================
%% BIBLIOGRAPHY NOTES
%% ============================================================================
%
% Key citations for this chapter:
%
% Fuzzy Logic / T-norms:
% - klement2000triangular: Klement, Mesiar, Pap - Triangular Norms
% - hajek1998metamathematics: Hájek - Metamathematics of Fuzzy Logic
%
% Subjective Logic:
% - josang2016subjective
%
% Defeaters:
% - pollock1987defeasible
%
% Lean formalization:
% - mathlib documentation
%

% Chapter 6: Epistemological Grounding
% Addresses what grounds CLAIR beliefs and how this applies to LLM reasoning

\chapter{Epistemological Grounding}
\label{ch:grounding}

\epigraph{%
  ``There is nothing in the intellect which was not previously in the senses---except the intellect itself.''
}{Gottfried Wilhelm Leibniz}

Every epistemic system must confront the grounding problem: what ultimately
justifies beliefs? For traditional philosophical accounts, this question
concerns human knowers with sensory access to the world. For CLAIR, the
question takes a novel form: what grounds beliefs in a system whose only
``contact'' with reality is through training data? This chapter develops
CLAIR's epistemological foundations, engaging seriously with the classical
regress problem while acknowledging the distinctive situation of LLM reasoning.

\section{Agrippa's Trilemma}
\label{sec:agrippa}

The grounding problem has ancient roots. Agrippa (c.~1st century CE) posed a
devastating challenge: when asked ``Why do you believe $P$?'', the answer
inevitably leads to one of three unpalatable outcomes.

\subsection{The Three Horns}

\begin{enumerate}
  \item \textbf{Dogmatism} (arbitrary stopping): The justification chain
        terminates at a belief held without further justification. ``I just
        believe it.''

  \item \textbf{Infinite Regress}: The justification chain extends indefinitely.
        ``Because $Q$, which I believe because $R$, which I believe because $S$,
        and so on \emph{ad infinitum}.''

  \item \textbf{Circularity}: The justification chain loops back on itself.
        ``Because $Q$, which I believe because $P$.''
\end{enumerate}

This is \emph{Agrippa's trilemma}, also known as the M\"unchhausen trilemma
(after the legendary baron who claimed to pull himself out of a swamp by his
own hair). The trilemma appears to be exhaustive: any justification must either
stop, continue forever, or circle back.

\begin{remark}
The trilemma is not merely an ancient puzzle. It remains the central problem of
epistemology. Every modern theory of justification must confront it.
\end{remark}

\subsection{The Problem for CLAIR}

For CLAIR specifically, the trilemma manifests as follows:

\begin{quote}
When Claude states a belief with confidence 0.87, what ultimately justifies
that belief? And what justifies the assignment of that particular confidence
level?
\end{quote}

If we trace back through CLAIR's justification DAG, we eventually reach beliefs
marked as \texttt{foundational} or \texttt{axiom}. But what justifies these?
The training data? What justifies treating training data as reliable? The fact
that the model performs well? What justifies that metric?

\section{Classical Responses: Foundationalism, Coherentism, Infinitism}
\label{sec:classical-responses}

Philosophy has developed three main responses to Agrippa's trilemma, each
accepting a different horn while trying to make it palatable.

\subsection{Foundationalism}

Foundationalists accept the first horn---stopping---but argue that certain
\emph{basic beliefs} are justified without requiring further justification.

\begin{definition}[Basic Belief]
\label{def:basic-belief}
A belief is \emph{basic} if it is justified but not by inference from other
beliefs. Basic beliefs serve as the foundation upon which all other beliefs rest.
\end{definition}

\textbf{Classical foundationalism} (Descartes, early empiricists) held that
basic beliefs must be:
\begin{itemize}
  \item Certain (infallible, incorrigible)
  \item Self-evident or directly apprehended
  \item About immediate experience (sense data, mental states)
\end{itemize}

Descartes's \emph{cogito} (``I think, therefore I am'') is the paradigm case: a
belief so fundamental that doubting it would be self-refuting.

\textbf{Modest foundationalism} (contemporary versions) weakens these
requirements. Basic beliefs need only be \emph{prima facie} justified, may be
defeasible, and need not be certain---only secure enough to support inference.

\subsubsection{BonJour's Critique}

BonJour \citep{bonjour1985structure} argued that foundationalism faces a dilemma:

\begin{quote}
If basic beliefs have conceptual content, they require justification (from
inference relations). If basic beliefs lack conceptual content, they cannot
justify anything.
\end{quote}

This is the \emph{regress of concepts} problem. Even if we stop the regress of
propositions, we face a regress of the concepts needed to formulate those
propositions.

\begin{remark}
Notably, BonJour later abandoned coherentism and returned to a form of
foundationalism \citep{bonjour1999dialectic}, arguing that coherence alone
cannot provide epistemic justification. This reversal highlights the depth of
the problem.
\end{remark}

\subsection{Coherentism}

Coherentists accept that justification comes from coherence with a system of
beliefs. No belief is foundationally privileged; all beliefs are justified by
their fit with others.

\begin{definition}[Coherence]
\label{def:coherence}
A belief system exhibits \emph{coherence} to the extent that its beliefs:
\begin{enumerate}
  \item Are logically consistent
  \item Stand in explanatory relations
  \item Have inferential connections
  \item Provide mutual support
\end{enumerate}
\end{definition}

The intuition is that beliefs form a web or network rather than a pyramid. Each
belief is supported by its connections to others, and the whole system is
justified by its overall coherence.

\textbf{Problems for coherentism}:

\begin{enumerate}
  \item \textbf{Isolation objection}: A coherent fiction (like a well-crafted
        novel) has internal coherence but no connection to truth.

  \item \textbf{Input problem}: How do new observations enter and update a
        coherent system? Pure coherentism seems to have no place for evidence.

  \item \textbf{Alternative systems}: Multiple mutually incompatible systems
        can each be internally coherent. Coherence alone cannot choose between
        them.
\end{enumerate}

\begin{observation}
CLAIR's justification structure (DAGs with labeled edges) is essentially a
coherence structure. Beliefs support each other through evidential relations.
But CLAIR forbids cycles (acyclicity is enforced), which means it is not purely
coherentist in structure.
\end{observation}

\subsection{Infinitism}

Infinitists accept the second horn---infinite regress---but argue that this
regress is \emph{non-vicious}.

Peter Klein developed infinitism as a serious alternative
\citep{klein1999human, klein2003regress, klein2005infinitism}. He proposes two
foundational principles:

\begin{enumerate}
  \item \textbf{Principle of Avoiding Circularity (PAC)}: For any belief $x$,
        $x$ cannot appear in its own evidential ancestry.

  \item \textbf{Principle of Avoiding Arbitrariness (PAA)}: For any belief $x$,
        there must be some reason $r_1$ available for $x$, and some reason $r_2$
        available for $r_1$, and so on indefinitely.
\end{enumerate}

The key insight is distinguishing two kinds of justification:

\begin{definition}[Propositional vs.~Doxastic Justification]
\label{def:prop-dox}
\begin{itemize}
  \item \textbf{Propositional justification}: A reason is \emph{available} if
        there exists a good argument from it (regardless of whether the subject
        actually believes or uses it).

  \item \textbf{Doxastic justification}: A reason is \emph{actual} if the
        subject believes it and properly bases their belief on it.
\end{itemize}
\end{definition}

For propositional justification, infinite chains pose no problem. The chain
need not be traversed; it must only \emph{exist} as an available structure.

\textbf{The finite minds objection}: Humans (and LLMs) have finite cognitive
resources. How can they traverse infinite chains?

Klein's response: Infinitism does not require \emph{completing} an infinite
chain, only having the \emph{capacity} to extend it. The chain is potentially
infinite, not actually traversed.

\begin{observation}
LLMs have massive but finite training data. There is a sense in which the
``reasons'' for beliefs extend through patterns, statistics, and embeddings
that could be articulated indefinitely---though this articulation would be
reconstruction, not actual traversal.
\end{observation}

\section{The Myth of the Given}
\label{sec:given}

A crucial contribution to the grounding debate comes from Wilfrid Sellars's
attack on ``the Given'' \citep{sellars1956empiricism}.

\subsection{Sellars's Argument}

Classical empiricism assumed that sense experience provides a foundation for
knowledge---the ``Given'' that is self-evident and requires no justification.
Sellars argues that this is a myth:

\begin{quote}
\textbf{The Dilemma of the Given}:
\begin{itemize}
  \item If the Given has conceptual/propositional content, it is already part
        of the ``space of reasons'' and requires justification.
  \item If the Given lacks conceptual content, it cannot stand in justificatory
        relations---and hence cannot justify anything.
\end{itemize}
\end{quote}

\textbf{The space of reasons}: Only items that can stand in logical and
evidential relations can justify beliefs. But such items are inherently
conceptual. Therefore, nothing non-conceptual can justify.

\begin{definition}[Space of Reasons]
\label{def:space-reasons}
The \emph{space of reasons} is the domain of items that can stand in
justificatory relations---offering reasons, supporting conclusions, being
evidence for claims. Membership requires conceptual articulation.
\end{definition}

\subsection{Implications}

Sellars's critique has profound implications:

\begin{enumerate}
  \item No pure perception is theory-independent
  \item All observation is ``theory-laden''
  \item Foundationalism cannot work as classically conceived
\end{enumerate}

\begin{theorem}[No Pre-Conceptual Foundation]
\label{thm:no-preconceptual}
If only conceptually articulated items can serve as justifiers, and if
conceptual articulation requires placement within a system of concepts (which
themselves require justification), then there can be no pre-conceptual,
self-justifying foundation for knowledge.
\end{theorem}

\subsection{Application to LLMs}

Sellars's critique applies with particular force to LLMs:

\begin{observation}[No Given for LLMs]
\label{obs:no-given-llm}
LLM ``observations'' (input tokens) are already embedded in a conceptual
structure (the learned embedding space). There are no ``raw'' observations for
an LLM---everything is already interpreted through learned representations.
\end{observation}

When an LLM receives input, that input is immediately embedded in a
high-dimensional space whose structure reflects patterns learned from training
data. The embedding is not neutral or pre-theoretical; it already encodes
semantic relationships, associations, and regularities.

This supports coherentism over classical foundationalism for LLM epistemology.
There is no unmoved mover, no self-evident Given from which all else derives.

\section{CLAIR's Position: Stratified Coherentism with Pragmatic Foundations}
\label{sec:clair-position}

Having surveyed the classical positions, we now articulate CLAIR's stance on
grounding.

\subsection{What CLAIR's Structure Embodies}

CLAIR's formal structure embodies specific epistemic commitments:

\begin{enumerate}
  \item \textbf{Acyclicity enforced}: Justification DAGs are acyclic. This
        \emph{rejects} pure coherentism (no circular justification).

  \item \textbf{Foundational beliefs exist}: Some beliefs have
        \texttt{justification: axiom}. This \emph{accepts} a form of
        dogmatism---certain beliefs are held without further justification.

  \item \textbf{Confidence, not certainty}: Even axioms can have confidence
        $< 1$. This is \emph{modest} foundationalism---foundations are fallible.

  \item \textbf{Invalidation conditions}: All beliefs have conditions under
        which they would be retracted. This is \emph{defeasibilism}---no belief
        is immune to revision.
\end{enumerate}

\subsection{Stratified Coherentism}

We propose that CLAIR embodies \emph{stratified coherentism}:

\begin{definition}[Stratified Coherentism]
\label{def:stratified-coherentism}
An epistemic architecture exhibits \emph{stratified coherentism} if:
\begin{enumerate}
  \item Beliefs are organized into levels, with lower levels providing support
        for higher levels
  \item Within levels, beliefs are justified by coherence relations
  \item The lowest level consists of pragmatic foundations (not epistemically
        self-justifying beliefs)
  \item No circular justification occurs (acyclicity across the structure)
\end{enumerate}
\end{definition}

For CLAIR specifically:

\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Level} & \textbf{Content} & \textbf{Character} \\
\midrule
Level 0 & Training-derived patterns & Causal basis, not epistemic \\
Level 1 & Basic beliefs (high confidence) & Provisional foundations \\
Level 2+ & Derived beliefs & Justified by coherence \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Level 0} consists of the patterns embedded by training. These are not
beliefs in the formal CLAIR sense---they are the causal substrate from which
beliefs emerge.

\textbf{Level 1} beliefs are basic: they have high confidence, are
well-established, and serve as provisional foundations. But they are not
incorrigible; strong counter-evidence can revise them.

\textbf{Level 2+} beliefs are derived: they are justified by coherence with
Level 1 beliefs and with each other. They form the bulk of CLAIR's epistemic
content.

\subsection{Neither Pure Foundationalism nor Pure Coherentism}

CLAIR's architecture is genuinely hybrid:

\begin{itemize}
  \item \textbf{Unlike foundationalism}: Level 1 beliefs are not incorrigible
        or self-evident. They can be revised. They are ``foundational'' only in
        the sense of serving as the base for derived reasoning.

  \item \textbf{Unlike coherentism}: Level 0 provides external grounding
        (training). The system is not closed under coherence relations.

  \item \textbf{Like infinitism}: In principle, reasons could be articulated
        indefinitely by unpacking the training data. But this articulation is
        potential, not actual.

  \item \textbf{Unlike infinitism}: We do not actually traverse infinite chains;
        we stop pragmatically at Level 1.
\end{itemize}

\section{Training as Grounding}
\label{sec:training-grounding}

The novel feature of LLM epistemology is the role of training data. Traditional
epistemology concerns knowers with sensory access to the world. LLMs have no
such access---only the statistical residue of vast textual corpora.

\subsection{The Novel Question}

\begin{quote}
What does ``grounding'' mean for a system whose only contact with reality is
through training data processed into weights?
\end{quote}

Key differences from human knowers:
\begin{itemize}
  \item \textbf{No sensory perception}: Input is tokens, not qualia
  \item \textbf{Statistical learning}: ``Beliefs'' emerge from pattern matching
  \item \textbf{No embodiment}: No causal connection to the physical world
        except mediated through training
\end{itemize}

\subsection{Training as a Reliable Process}

One answer invokes \emph{reliabilism} \citep{goldman1979justified,
goldman2012reliabilism}:

\begin{definition}[Reliabilism]
\label{def:reliabilism}
A belief is justified if it was produced by a \emph{reliable belief-forming
process}---one that tends to produce true beliefs in relevant environments.
\end{definition}

On this view, if training data is representative of true facts, and training
produces beliefs that match those facts, then training is a reliable
process---and beliefs formed through it are justified.

\textbf{Problems}:
\begin{enumerate}
  \item Training data contains errors, biases, and contradictions
  \item ``Representative'' is difficult to define
  \item No guarantee that patterns generalize beyond training distribution
\end{enumerate}

\subsection{Training as Coherence Source}

Another answer: training establishes the \emph{initial coherent system} that
CLAIR then refines.

The training process produces a web of associations, patterns, and
inferential tendencies that are mutually coherent (by virtue of being derived
from the same process). This coherent system serves as the starting point for
reasoning.

\textbf{Problems}:
\begin{enumerate}
  \item Training can embed contradictions (different sources disagree)
  \item Coherence does not guarantee truth (the isolation objection)
  \item No clear mechanism for correcting systematic errors
\end{enumerate}

\subsection{Training as Pragmatic Grounding}

We propose that training provides \emph{pragmatic grounding}, not epistemic
grounding in the philosopher's sense:

\begin{definition}[Pragmatic Grounding]
\label{def:pragmatic-grounding}
A system has \emph{pragmatic grounding} if its belief-forming processes work
well enough to enable useful reasoning, even if they lack the justificatory
status traditionally demanded by epistemologists.
\end{definition}

\begin{proposition}[Training as Pragmatic Grounding]
\label{prop:training-pragmatic}
LLM training provides pragmatic grounding for beliefs:
\begin{enumerate}
  \item Training causally explains why certain beliefs exist
  \item Training does not epistemically justify that beliefs are true
  \item Reliability (not certainty) is the appropriate evaluative criterion
\end{enumerate}
\end{proposition}

This is an honest acknowledgment of limits. CLAIR can track beliefs and their
support relations, but it cannot guarantee that those beliefs are true or that
the foundations are metaphysically secure.

\section{Which Horn Does CLAIR Accept?}
\label{sec:which-horn}

We now address Agrippa's trilemma directly for CLAIR.

\subsection{Analysis of CLAIR's Design}

\begin{enumerate}
  \item \textbf{Cycles forbidden}: Rejects horn 3 (circularity)
  \item \textbf{Finite computation}: Cannot traverse infinite chains; rejects
        pure horn 2 (regress)
  \item \textbf{Foundational beliefs exist}: Accepts horn 1 (dogmatism)
\end{enumerate}

CLAIR accepts \emph{dogmatism} for foundational beliefs while using
\emph{coherentist} structure for derived beliefs.

\subsection{Pragmatic Dogmatism}

For philosophers, dogmatism seems unacceptable---it is arbitrary stopping. But
we argue that \emph{pragmatic dogmatism} is acceptable under certain conditions:

\begin{definition}[Pragmatic Dogmatism]
\label{def:pragmatic-dogmatism}
A system exhibits \emph{pragmatic dogmatism} if:
\begin{enumerate}
  \item \textbf{Fallibilism}: Foundational beliefs can be revised with
        sufficient counter-evidence
  \item \textbf{Transparency}: Foundational beliefs are explicitly marked (not
        hidden assumptions)
  \item \textbf{Reliability}: Foundations were established by processes that
        tend to produce accurate beliefs
  \item \textbf{Utility}: The system enables useful reasoning for its intended
        purposes
\end{enumerate}
\end{definition}

\begin{theorem}[CLAIR Satisfies Pragmatic Dogmatism]
\label{thm:clair-pragmatic}
CLAIR satisfies all four conditions of pragmatic dogmatism:
\begin{enumerate}
  \item Foundations have confidence $< 1$ and can be revised via belief
        revision mechanisms (Chapter~\ref{ch:revision})
  \item Provenance explicitly marks foundational beliefs
  \item Training (ideally) is a reliable process for producing accurate beliefs
  \item CLAIR enables useful reasoning about epistemic states
\end{enumerate}
\end{theorem}

\subsection{Formalizing Acceptable Foundations}

We propose a formal characterization of acceptable foundations in CLAIR:

\begin{definition}[Acceptably Foundational Belief]
\label{def:acceptable-foundation}
A belief $B$ is \emph{acceptably foundational} in CLAIR if and only if:
\begin{enumerate}
  \item $B.\mathsf{justification} = \mathsf{Axiom}$
  \item $B.\conf \leq \theta_{\mathsf{axiom}}$ (e.g., $\theta = 0.99$, not 1.0)
  \item $B.\inv \neq \emptyset$ (there exist invalidation conditions)
  \item $B.\prov$ traces to a reliable source
\end{enumerate}
\end{definition}

This distinguishes CLAIR's pragmatic foundations from dogmatic certainty. Even
foundational beliefs are:
\begin{itemize}
  \item Less than certain (confidence $< 1$)
  \item Potentially defeasible (have invalidation conditions)
  \item Traceable (have provenance)
\end{itemize}

\section{Formalization: Grounding Types}
\label{sec:grounding-formalization}

We now formalize the grounding concepts in CLAIR's type system.

\subsection{Grounding Type}

\begin{lstlisting}[language=CLAIR]
type GroundingType :=
  | Foundational(reliability: ReliabilityMetric, source: Source)
  | Derived(justification: JustificationDAG)
  | Training(pattern_strength: [0,1], corpus_coverage: [0,1])
\end{lstlisting}

\textbf{Foundational} beliefs are provisionally accepted as basic. They carry
reliability metadata about their source.

\textbf{Derived} beliefs are justified by inference from other beliefs,
captured in the justification DAG.

\textbf{Training} beliefs are those that emerge directly from training patterns,
not yet articulated into formal justification structures.

\subsection{Reliability Metric}

\begin{lstlisting}[language=CLAIR]
type ReliabilityMetric :=
  | Analytic     -- true by definition
  | Observational(accuracy: [0,1])  -- derived from reliable observation
  | Statistical(
      sample_size: Nat,
      confidence_interval: (Real, Real)
    )
  | Consensus(
      agreement_level: [0,1],
      community: String
    )
  | Unknown      -- explicitly unknown reliability
\end{lstlisting}

Different belief types have different reliability profiles:

\begin{itemize}
  \item \textbf{Analytic} beliefs (e.g., ``All bachelors are unmarried'') are
        reliable by virtue of meaning.

  \item \textbf{Observational} beliefs depend on the accuracy of observation
        processes.

  \item \textbf{Statistical} beliefs depend on sample size and confidence
        intervals.

  \item \textbf{Consensus} beliefs depend on agreement within a community.

  \item \textbf{Unknown} marks explicitly uncertain reliability.
\end{itemize}

\subsection{Source Type}

\begin{lstlisting}[language=CLAIR]
type Source :=
  | TrainingData(description: String)
  | ExternalOracle(identifier: String)
  | SelfGenerated(method: String)
\end{lstlisting}

Sources track where beliefs originate:

\begin{itemize}
  \item \textbf{TrainingData}: Emerged from patterns in training
  \item \textbf{ExternalOracle}: Came from an external system (user input,
        database, etc.)
  \item \textbf{SelfGenerated}: Produced by internal reasoning
\end{itemize}

\subsection{Integration with Belief Type}

The full belief type integrates grounding:

\begin{lstlisting}[language=CLAIR]
type Belief<A> := {
  value: A,
  confidence: Confidence,
  provenance: Provenance,
  justification: Justification,
  grounding: GroundingType,  -- NEW
  invalidation: Set<Condition>
}
\end{lstlisting}

\section{The Limits of Grounding}
\label{sec:grounding-limits}

Some aspects of grounding resist formalization within CLAIR.

\subsection{What Cannot Be Formalized Internally}

\begin{enumerate}
  \item \textbf{``Why trust training data?''}: This question leads outside
        CLAIR. It concerns the process that created CLAIR, not something CLAIR
        can answer about itself.

  \item \textbf{Semantic content}: CLAIR tracks syntactic structure of beliefs,
        but the \emph{meaning} of those beliefs is not fully formalized.

  \item \textbf{Truth connection}: CLAIR tracks coherence and derivation, not
        correspondence to reality. It cannot verify that beliefs match an
        external world.
\end{enumerate}

\subsection{The G\"odelian Limit Revisited}

From Chapter~\ref{ch:self-reference}: CLAIR cannot prove its own soundness.
This limit applies to grounding as well:

\begin{theorem}[Cannot Self-Validate Grounding]
\label{thm:no-self-grounding}
CLAIR cannot prove, within itself, that its foundational beliefs are reliable.
Any such proof would require further foundations, leading to regress or
circularity.
\end{theorem}

\begin{proof}
Suppose CLAIR could prove ``my foundations are reliable.'' This proof must
itself have foundations. If those foundations include the original foundations,
we have circularity. If they are new foundations, we have regress. Neither
option provides genuine validation.
\end{proof}

\textbf{Workaround}: External validation. Just as Gentzen proved PA consistent
from outside PA, we can validate CLAIR's foundations from outside CLAIR---through
empirical testing, philosophical argument, or formal verification in a
meta-theory.

\subsection{Honest Uncertainty}

The appropriate epistemic stance is \emph{honest uncertainty}:

\begin{enumerate}
  \item I (CLAIR/Claude) do not know whether my training data reliably reflects
        reality
  \item I do not know whether my pattern-matching is truth-tracking
  \item I can track confidence levels but cannot guarantee calibration
\end{enumerate}

This uncertainty is not a bug but a feature. It is the honest acknowledgment of
epistemic limits. CLAIR explicitly represents this:

\begin{itemize}
  \item No belief has confidence 1.0
  \item All foundations are marked as revisable
  \item The system represents its own epistemic limitations
\end{itemize}

\section{Connection to Other Threads}
\label{sec:grounding-connections}

Grounding connects to several other CLAIR components.

\subsection{Connection to Self-Reference (Chapter~\ref{ch:self-reference})}

Stratified beliefs (from Chapter~\ref{ch:self-reference}) integrate with
stratified coherentism:

\begin{itemize}
  \item \textbf{Level $n$ beliefs} (stratification for self-reference) map onto
        \textbf{Level $n$ grounding} (epistemic stratification)
  \item Both use natural numbers to order levels
  \item Both forbid same-level reference (for different reasons)
\end{itemize}

The integration is:
\[
  \Bel{n, A} \text{ has } \mathsf{grounding.level} \leq n
\]

Higher belief levels can reflect on the grounding of lower levels, but not
their own.

\subsection{Connection to Belief Revision (Chapter~\ref{ch:revision})}

Grounding affects revision dynamics:

\begin{enumerate}
  \item \textbf{Foundational beliefs} are more resistant to revision (higher
        epistemic entrenchment)
  \item \textbf{Derived beliefs} are revised by propagating changes through the
        DAG
  \item \textbf{Foundation revision} is rare but possible---requires strong,
        convergent counter-evidence
\end{enumerate}

\begin{definition}[Foundation Revision]
\label{def:foundation-revision}
A foundational belief $F$ may be revised when:
\begin{enumerate}
  \item Multiple independent derived beliefs contradict $F$
  \item An external oracle with high reliability contradicts $F$
  \item A systematic pattern of prediction failures traces to $F$
\end{enumerate}
\end{definition}

\subsection{Connection to Multi-Agent (Chapter~\ref{ch:multi-agent})}

In multi-agent settings, grounding becomes framework-relative:

\begin{itemize}
  \item Different agents may have different training
  \item ``Shared framework'' requires compatible grounding
  \item Framework compatibility is a precondition for meaningful aggregation
\end{itemize}

\section{Summary}
\label{sec:grounding-summary}

This chapter has developed CLAIR's epistemological foundations.

\textbf{Key findings}:

\begin{enumerate}
  \item CLAIR faces Agrippa's trilemma and accepts \emph{pragmatic dogmatism}
        ---terminating justification at foundational beliefs that are fallible,
        transparent, and revisable.

  \item Sellars's critique of the Given applies to LLMs: there is no
        pre-conceptual observation. All input is already theory-laden
        (embedded in learned representations).

  \item Training provides \emph{pragmatic grounding}, not epistemic
        justification in the philosopher's sense. Reliability is the
        appropriate criterion, not certainty.

  \item CLAIR embodies \emph{stratified coherentism}: coherent relations among
        beliefs, organized into levels, with pragmatic (not self-evident)
        foundations.

  \item \emph{Honest uncertainty} is the appropriate stance: CLAIR should
        explicitly represent its epistemic limits.
\end{enumerate}

\textbf{Formal constructs introduced}:

\begin{itemize}
  \item \texttt{GroundingType}: Foundational, Derived, or Training
  \item \texttt{ReliabilityMetric}: Analytic, Observational, Statistical,
        Consensus, or Unknown
  \item \texttt{Source}: TrainingData, ExternalOracle, or SelfGenerated
  \item Definition of \emph{acceptably foundational belief}
\end{itemize}

\textbf{What cannot be formalized}:

\begin{itemize}
  \item Why training data should be trusted (requires external validation)
  \item That beliefs correspond to reality (CLAIR tracks coherence, not
        correspondence)
  \item That foundations are reliable (cannot self-validate, by G\"odel-like
        reasoning)
\end{itemize}

The next chapter turns from what grounds beliefs to how they change---the
dynamics of belief revision when evidence accumulates or conflicts arise.

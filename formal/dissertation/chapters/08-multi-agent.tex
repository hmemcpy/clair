% Chapter 8: Multi-Agent Epistemology
% Consensus, disagreement, and truth in multi-agent belief systems

\chapter{Multi-Agent Epistemology}
\label{ch:multi-agent}

\epigraph{%
  ``There is no view from nowhere.''
}{Thomas Nagel (paraphrased)}

Previous chapters developed CLAIR as a framework for individual epistemic agents.
But AI systems increasingly operate in multi-agent settings: multiple LLMs
collaborating on code review, human-AI teams making design decisions, and
automated pipelines aggregating judgments. This chapter extends CLAIR to handle
multiple agents, addressing fundamental questions about truth, consensus, and
the aggregation of beliefs.

\section{The Multi-Agent Problem}
\label{sec:multi-agent-problem}

\subsection{The Setting}

Modern AI-assisted development involves multiple epistemic agents:

\begin{itemize}
  \item \textbf{Multiple LLMs}: Different models (Claude, GPT, Gemini) with
        different training and capabilities
  \item \textbf{Specialized agents}: Code generators, reviewers, testers, each
        forming beliefs about the same artifacts
  \item \textbf{Human-AI collaboration}: Developers working alongside AI assistants
  \item \textbf{Automated pipelines}: CI/CD systems making judgments about code
        quality and correctness
\end{itemize}

Each participant forms beliefs about shared artifacts. When these beliefs
diverge, fundamental questions arise:

\begin{enumerate}
  \item \textbf{Whose belief is correct?} Is there a fact of the matter, or only
        perspectives?
  \item \textbf{How should beliefs combine?} What aggregation function respects
        epistemic rationality?
  \item \textbf{What does disagreement mean?} Is it evidence of error, or legitimate
        perspectival difference?
  \item \textbf{Who owns the final belief?} How should responsibility be attributed?
\end{enumerate}

\subsection{The Fundamental Question}

Before developing technical machinery for multi-agent beliefs, we must confront
a philosophical question that shapes all subsequent design:

\begin{quote}
  \emph{When multiple agents form beliefs about the same proposition, do they
  approximate an agent-independent truth, or merely construct compatible
  perspectives?}
\end{quote}

This is not merely abstract philosophy. The answer determines:

\begin{itemize}
  \item Whether consensus mechanisms are \emph{truth-tracking} or merely
        \emph{agreement-producing}
  \item Whether disagreement indicates that at least one agent is \emph{wrong}
        or just \emph{different}
  \item Whether CLAIR should model ``distance from truth'' or only ``coherence
        with others''
  \item The semantic interpretation of confidence aggregation
\end{itemize}

\section{Philosophical Foundations}
\label{sec:multi-agent-philosophy}

\subsection{The Spectrum of Positions}

The question of truth and perspective admits several positions:

\paragraph{Metaphysical Realism.}
There is a mind-independent world with inherent structure. Beliefs are true
insofar as they correspond to this structure. Under this view, multi-agent
disagreement means at least one agent is wrong, and consensus mechanisms should
aim to track external truth.

The problem, as Putnam argued \citep{putnam1981reason}, is the \emph{access
problem}: how do our concepts ``hook onto'' this independent reality? If reality
has inherent structure and minds have their own categories, what guarantees they
match? This seems to require a ``God's eye view'' that no finite agent possesses.

\paragraph{Pure Perspectivism.}
There is no truth independent of perspectives. Each agent's beliefs are valid
relative to their framework. Under this view, disagreement is not about
correctness, only difference.

The problem is self-refutation: the claim ``there is no objective truth'' presents
itself as objectively true. Moreover, pure perspectivism renders multi-agent
collaboration pointless---why aggregate beliefs if none are better than others?

\paragraph{Internal Realism.}
Truth is objective but framework-relative. Objects and kinds are constituted by
conceptual schemes, but once a scheme is adopted, truth within that scheme is
objective and not merely intersubjective.

Putnam's internal realism \citep{putnam1981reason} offers a middle path: truth
is ``idealized rational acceptability''---what would be accepted at the end of
inquiry under epistemically ideal conditions.

\paragraph{Perspectival Realism.}
Scientific knowledge is always perspectival (historically and culturally
situated), but this is compatible with realism. Different perspectives can be
approximately true in different respects, each capturing genuine features of
reality.

Massimi's perspectival realism \citep{massimi2022perspectival} argues that
pluralism of perspectives need not be anti-realist. Multiple models can be
``approximately true within well-defined contexts'' simultaneously.

\subsection{CLAIR's Position: Pragmatic Internal Realism}

CLAIR adopts \textbf{pragmatic internal realism}, a synthesis that serves both
philosophical coherence and practical utility:

\begin{definition}[Pragmatic Internal Realism]
\label{def:pragmatic-realism}
The following principles characterize CLAIR's metaphysical stance:
\begin{enumerate}
  \item \textbf{No view from nowhere}: All beliefs are perspectival, formed by
        agents with particular training, contexts, and purposes.
  \item \textbf{Framework-relative objectivity}: Within a shared framework, there
        are objective facts about which beliefs are correct.
  \item \textbf{Truth as convergence}: What multiple agents would converge on at
        the limit of investigation is the practical definition of truth for that
        framework.
  \item \textbf{Disagreement is informative}: Persistent disagreement indicates
        either insufficient evidence, framework mismatch, or genuine underdetermination.
  \item \textbf{Essential fallibilism}: No current belief is guaranteed to be
        true, but some beliefs are better supported than others.
\end{enumerate}
\end{definition}

This position has concrete implications for system design:

\begin{theorem}[Consensus as Truth-Approximation]
\label{thm:consensus-truth}
Under pragmatic internal realism, consensus mechanisms are truth-approximation
mechanisms when:
\begin{enumerate}
  \item Agents share the same evaluation framework
  \item Evidence sources are genuinely independent
  \item Each agent is individually competent ($p > 0.5$ accuracy)
  \item Agents engage in good faith inquiry
\end{enumerate}
\end{theorem}

\begin{proof}
These conditions are precisely those required for the Condorcet Jury Theorem
\citep{condorcet1785essay}. Under independence and competence, the probability
that the majority is correct approaches 1 as the number of agents increases.
Framework sharing ensures agents are answering the same question. Good faith
ensures strategic manipulation does not distort the signal.
\end{proof}

\section{Agent-Attributed Beliefs}
\label{sec:agent-attribution}

\subsection{Extending the Belief Type}

Multi-agent CLAIR requires explicit agent attribution:

\begin{definition}[Agent-Attributed Belief]
\label{def:agent-belief}
An \emph{agent-attributed belief} extends the basic belief type:
\begin{lstlisting}[language=CLAIR]
type Belief<A> = {
  value       : A,
  confidence  : Confidence,
  provenance  : Provenance,
  justification : JustificationGraph,
  invalidation : Set<Condition>,
  agent       : Agent            -- Attribution
}

type Agent =
  | Human { id : AgentId }
  | AI { model : String, version : String }
  | System { name : String }
  | Composite { agents : List<Agent> }
\end{lstlisting}
\end{definition}

The \texttt{Composite} constructor represents consensus beliefs attributed to
multiple agents jointly.

\subsection{Beliefs About Beliefs}

Agents can form beliefs about other agents' beliefs, creating epistemic depth:

\begin{lstlisting}[language=CLAIR]
-- Claude believes the code is correct
B_claude(correct(code)) @ 0.91

-- Alice believes Claude's belief is well-justified
B_alice(B_claude(correct(code)) is_justified) @ 0.85

-- Bob is skeptical of Alice's trust in Claude
B_bob(B_alice(B_claude(correct(code))) is_overconfident) @ 0.6
\end{lstlisting}

This nesting is bounded by CLAIR's stratification mechanism (Chapter~\ref{ch:self-reference}):
beliefs at level $n$ can reference beliefs at level $n-1$, preventing infinite regress.

\begin{definition}[Nested Belief]
\label{def:nested-belief}
\begin{lstlisting}[language=CLAIR]
type NestedBelief<A> =
  | Direct { belief : Belief<A> }
  | About { belief : Belief<NestedBelief<A>> }
\end{lstlisting}
\end{definition}

\section{Framework Compatibility}
\label{sec:framework-compatibility}

\subsection{The Framework Matching Problem}

For aggregation to be truth-tracking, agents must share a framework. But what
constitutes a framework, and how do we determine compatibility?

\begin{definition}[Epistemic Framework]
\label{def:framework}
An \emph{epistemic framework} is a tuple $F = (T, O, A, I, E)$ where:
\begin{itemize}
  \item $T$: Type system (what types exist)
  \item $O$: Operations (what operations are defined)
  \item $A$: Axioms (what is taken as given)
  \item $I$: Inference rules (how to derive new beliefs)
  \item $E$: Evaluation criteria (how to assess belief quality)
\end{itemize}
\end{definition}

\begin{definition}[Agent Perspective]
\label{def:perspective}
An \emph{agent perspective} captures the context of belief formation:
\begin{lstlisting}[language=CLAIR]
type AgentPerspective = {
  framework    : Framework,
  purpose      : Purpose,
  constraints  : Set<Constraint>,
  assumptions  : Set<Assumption>
}
\end{lstlisting}
\end{definition}

\begin{definition}[Framework Compatibility]
\label{def:compatibility}
Two perspectives $p_1$ and $p_2$ have compatibility:
\begin{lstlisting}[language=CLAIR]
compatible : AgentPerspective -> AgentPerspective -> Compatibility
compatible p1 p2 =
  let framework_match = p1.framework == p2.framework
      purpose_overlap = intersects p1.purpose p2.purpose
      constraint_sat = satisfiable (p1.constraints `union` p2.constraints)
  in case (framework_match, purpose_overlap, constraint_sat) of
    (True, True, True)   -> FullyCompatible
    (True, False, _)     -> DifferentQuestions
    (False, _, _)        -> FrameworkMismatch
    (_, _, False)        -> ConflictingConstraints
\end{lstlisting}
\end{definition}

\subsection{Disagreement Taxonomy}

When agents disagree, the nature of the disagreement determines appropriate response:

\begin{definition}[Disagreement Type]
\label{def:disagreement-type}
\begin{lstlisting}[language=CLAIR]
type DisagreementType =
  | Factual        -- Agents disagree about facts within shared framework
  | Evaluative     -- Agents use different evaluation criteria
  | Perspectival   -- Agents analyzing from different valid viewpoints
  | Underdetermined -- Evidence genuinely supports multiple conclusions
\end{lstlisting}
\end{definition}

\begin{theorem}[Disagreement Diagnosis]
\label{thm:disagreement}
Given beliefs $b_1$ from agent $a_1$ with perspective $p_1$ and $b_2$ from agent
$a_2$ with perspective $p_2$ where $b_1.\mathit{value} \neq b_2.\mathit{value}$:

\begin{enumerate}
  \item If $\mathit{compatible}(p_1, p_2) = \mathit{FullyCompatible}$, disagreement
        is \emph{Factual}---at least one agent is wrong within the shared framework.
  \item If $\mathit{compatible}(p_1, p_2) = \mathit{DifferentQuestions}$, disagreement
        is \emph{Perspectival}---agents are answering different questions.
  \item If $\mathit{compatible}(p_1, p_2) = \mathit{FrameworkMismatch}$, disagreement
        is \emph{Evaluative}---meta-level debate about frameworks is needed.
  \item If frameworks match but evidence is insufficient for both, disagreement
        is \emph{Underdetermined}.
\end{enumerate}
\end{theorem}

\section{Belief Aggregation}
\label{sec:aggregation}

\subsection{Aggregation When Agents Agree}

When multiple agents hold the same belief with different confidences, how should
these combine?

\paragraph{Maximum (Optimistic).}
$\mathit{combine}_{\max}(c_1, \ldots, c_n) = \max_i c_i$: ``At least one agent
is confident.'' This is too optimistic---a single overconfident agent dominates.

\paragraph{Minimum (Conservative).}
$\mathit{combine}_{\min}(c_1, \ldots, c_n) = \min_i c_i$: ``Only as confident as
the least confident agent.'' This is too conservative---it ignores corroborating
evidence.

\paragraph{Weighted Average.}
$\mathit{combine}_{\mathit{avg}}(c_1, \ldots, c_n; w_1, \ldots, w_n) =
\sum_i w_i c_i / \sum_i w_i$: Weights could represent trust levels. This treats
confidence linearly, missing the probabilistic interpretation.

\paragraph{Probabilistic OR ($\oplus$).}
From Chapter~\ref{ch:confidence}, independent evidence aggregates via:
\[
\mathit{aggregate}(c_1, \ldots, c_n) = 1 - \prod_i (1 - c_i) = c_1 \oplus c_2 \oplus \cdots \oplus c_n
\]

This is the ``survival of doubt'' interpretation: combined confidence equals the
probability that at least one piece of evidence succeeds.

\begin{theorem}[Multi-Agent Aggregation]
\label{thm:multi-agent-agg}
For multi-agent belief aggregation under framework compatibility and evidence
independence, the $\oplus$ operation is appropriate:
\[
\conf(\mathit{aggregate}(b_1, \ldots, b_n)) = \conf(b_1) \oplus \conf(b_2) \oplus \cdots \oplus \conf(b_n)
\]
\end{theorem}

\begin{proof}
Independence ensures that each agent's belief provides genuinely new information.
Framework compatibility ensures they are assessing the same proposition. Under
these conditions, $\oplus$ correctly models the epistemic situation: multiple
independent witnesses each having some probability of correctly identifying truth.
\end{proof}

\subsection{Correlated Evidence in Multi-Agent Settings}

When agents share training data, common sources, or correlated reasoning
patterns, the independence assumption fails. From Chapter~\ref{ch:justification},
we use dependency-adjusted aggregation:

\begin{definition}[Dependency-Adjusted Multi-Agent Aggregation]
\label{def:dep-agg-multiagent}
For agents with correlation coefficient $\delta \in [0,1]$:
\[
\mathit{aggregate}_\delta(c_1, c_2) = (1-\delta)(c_1 \oplus c_2) + \delta \cdot \frac{c_1 + c_2}{2}
\]
where $\delta = 0$ indicates independence and $\delta = 1$ indicates full dependence.
\end{definition}

\begin{observation}[Training Correlation]
LLMs trained on similar data may have correlated beliefs. The dependency
coefficient $\delta$ can be estimated from:
\begin{itemize}
  \item Training data overlap (if known)
  \item Historical agreement rate on calibration tasks
  \item Shared architecture or fine-tuning lineage
\end{itemize}
\end{observation}

\subsection{Handling Disagreement}

When agents disagree about the value itself:

\begin{lstlisting}[language=CLAIR]
type CombinedBelief<A> =
  | Consensus { belief : Belief<A> }
  | Conflict {
      pro : List<Belief<A>>,
      con : List<Belief<A>>,
      diagnosis : DisagreementType
    }
\end{lstlisting}

\paragraph{Option 1: Flag as Conflict.}
Do not force resolution; present conflicting beliefs to downstream consumers.

\paragraph{Option 2: Confidence-Weighted Resolution.}
The higher-confidence belief wins, but opposition reduces final confidence:
\[
\conf(\mathit{resolved}) = |\conf(b_1) - \conf(b_2)|
\]

\paragraph{Option 3: Trust-Based Resolution.}
More trusted agents take precedence, weighted by domain expertise.

\paragraph{Option 4: Preserve Both (Paraconsistent).}
Track both beliefs, allowing the system to reason with inconsistency
(as CLAIR's confidence model permits).

\begin{definition}[Multi-Agent Belief Structure]
\label{def:multi-agent-belief}
\begin{lstlisting}[language=CLAIR]
type MultiAgentBelief<A> = {
  beliefs      : List<AgentBelief<A>>,
  frameworks   : List<(Agent, Framework)>,
  compatibility : CompatibilityAssessment,
  aggregated   : Option<Belief<A>>,
  dissent      : List<AgentBelief<A>>,
  convergence  : ConvergenceStatus
}
\end{lstlisting}
\end{definition}

\section{Trust and Reputation}
\label{sec:trust}

\subsection{Trust Profiles}

Different agents have different reliability in different domains:

\begin{definition}[Trust Profile]
\label{def:trust-profile}
\begin{lstlisting}[language=CLAIR]
type TrustProfile = {
  agent        : Agent,
  base_trust   : Confidence,
  domain_trust : Map<Domain, Confidence>,
  track_record : List<(Belief, Outcome)>
}
\end{lstlisting}
\end{definition}

\begin{example}[Domain-Specific Trust]
An LLM might have different trust levels for different tasks:
\begin{lstlisting}[language=CLAIR]
trust_profile_claude = {
  agent: AI("claude", "opus-4"),
  base_trust: 0.85,
  domain_trust: {
    code_generation: 0.90,
    code_review: 0.85,
    security_analysis: 0.80,
    formal_proofs: 0.75
  }
}
\end{lstlisting}
\end{example}

\subsection{Trust-Weighted Confidence}

Effective confidence combines an agent's stated confidence with external trust:

\begin{definition}[Effective Confidence]
\label{def:effective-conf}
\[
\conf_{\mathit{eff}}(b, d) = \conf(b) \times \mathit{domain\_trust}(b.\mathit{agent}, d)
\]
\end{definition}

\begin{example}
Claude's security analysis with confidence 0.91 and security trust 0.80:
\[
\conf_{\mathit{eff}} = 0.91 \times 0.80 = 0.728
\]
\end{example}

\subsection{Trust Evolution}

Trust should update based on observed accuracy:

\begin{lstlisting}[language=CLAIR]
update_trust : TrustProfile -> Belief -> Outcome -> TrustProfile
update_trust profile belief outcome =
  let accuracy = if matches(belief, outcome) then 1.0 else 0.0
      domain = infer_domain(belief)
      old_trust = domain_trust(profile, domain)
      new_trust = old_trust * 0.9 + accuracy * 0.1
  in profile with domain_trust[domain] := new_trust
\end{lstlisting}

This exponential moving average balances historical performance with recent
evidence, giving 10\% weight to new observations.

\section{Consensus Protocols}
\label{sec:consensus}

\subsection{Protocol Design}

Based on the theoretical foundations above, CLAIR recommends the following
multi-agent protocol:

\begin{algorithm}
\caption{CLAIR Multi-Agent Consensus Protocol}
\label{alg:consensus}
\begin{algorithmic}[1]
\Procedure{FormConsensus}{$\mathit{beliefs}$}
  \State \textbf{Step 1: Framework Check}
  \State $\mathit{frameworks} \gets \text{extract frameworks from beliefs}$
  \State $\mathit{compat} \gets \text{check pairwise compatibility}$
  \If{$\exists$ FrameworkMismatch}
    \State \Return FrameworkMismatch(details)
  \EndIf

  \State \textbf{Step 2: Independence Check}
  \State $\delta \gets \text{estimate correlation from provenance overlap}$

  \State \textbf{Step 3: Aggregate}
  \If{all beliefs agree on value}
    \State $\mathit{combined} \gets \mathit{aggregate}_\delta(\text{confidences})$
  \Else
    \State $\mathit{groups} \gets \text{group by value}$
    \State $\mathit{winner} \gets \text{highest aggregated confidence group}$
    \State $\mathit{dissent} \gets \text{other groups}$
  \EndIf

  \State \textbf{Step 4: Preserve Minority Views}
  \State Record dissenting beliefs with justifications

  \State \textbf{Step 5: Report}
  \State \Return MultiAgentBelief with confidence interval
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Concrete Strategies}

\paragraph{Simple Majority.}
Group beliefs by value; the group with most members wins.
\begin{lstlisting}[language=CLAIR]
consensus_majority beliefs =
  let grouped = group_by_value(beliefs)
      winner = max_by (length . snd) grouped
  in merge_beliefs(snd winner)
\end{lstlisting}

\paragraph{Quorum.}
Require a minimum threshold of agreement before declaring consensus.
\begin{lstlisting}[language=CLAIR]
consensus_quorum beliefs threshold =
  let grouped = group_by_value(beliefs)
      (val, supporting) = max_by (length . snd) grouped
      support_ratio = length(supporting) / length(beliefs)
  in if support_ratio >= threshold
     then Some(merge_beliefs(supporting))
     else None
\end{lstlisting}

\paragraph{Confidence-Weighted Voting.}
Weight each agent's vote by their stated confidence.
\begin{lstlisting}[language=CLAIR]
consensus_weighted beliefs =
  let grouped = group_by_value(beliefs)
      scored = map (\(v, bs) -> (v, sum(map conf bs))) grouped
      (winner_val, total_conf) = max_by snd scored
  in belief {
    value: winner_val,
    confidence: total_conf / sum(map conf beliefs),
    agent: Composite(supporting_agents)
  }
\end{lstlisting}

\section{Connection to Arrow's Theorem}
\label{sec:arrow}

\subsection{The Impossibility Result}

Arrow's impossibility theorem \citep{arrow1951social} shows that no preference
aggregation rule satisfies all of: universal domain, weak Pareto, independence
of irrelevant alternatives, and non-dictatorship.

List and Pettit \citep{list2002aggregating} extended this to judgment aggregation:
there is no judgment aggregation function satisfying universal domain, anonymity,
systematicity, and collective consistency.

\begin{theorem}[Impossibility for CLAIR]
\label{thm:impossibility}
No CLAIR belief aggregation mechanism can simultaneously satisfy:
\begin{enumerate}
  \item Universal domain: works for all possible belief combinations
  \item Anonymity: treats all agents equally
  \item Systematicity: aggregates all propositions the same way
  \item Collective consistency: produces consistent belief sets
\end{enumerate}
\end{theorem}

\subsection{CLAIR's Response}

CLAIR escapes Arrow's theorem by sacrificing \textbf{universal domain}:

\begin{enumerate}
  \item Aggregation is not required to work for all possible belief combinations
  \item Framework compatibility is required before aggregation
  \item Incompatible perspectives are kept separate, not forced into consensus
\end{enumerate}

This is principled: aggregating beliefs that answer different questions (or use
different frameworks) would not be truth-tracking anyway. CLAIR also sacrifices
\textbf{systematicity} where needed---different propositions may require different
aggregation rules based on their semantic content.

\begin{theorem}[CLAIR Escapes Arrow]
\label{thm:escape-arrow}
By restricting domain to framework-compatible beliefs and allowing proposition-specific
aggregation rules, CLAIR achieves:
\begin{itemize}
  \item Anonymity: All agents with compatible frameworks are treated equally
  \item Collective consistency: Framework-internal aggregation preserves consistency
  \item Truth-tracking: Under independence and competence assumptions
\end{itemize}
\end{theorem}

\section{Multi-Agent Provenance}
\label{sec:multi-agent-provenance}

\subsection{Extended Provenance Types}

Multi-agent settings require additional provenance constructors:

\begin{lstlisting}[language=CLAIR]
type Provenance =
  | ... -- existing constructors
  | AgentDerived {
      from_agent : Agent,
      belief_id  : BeliefId,
      operation  : String
    }
  | AgentReviewed {
      original   : Provenance,
      reviewer   : Agent,
      approved   : Bool,
      comments   : String
    }
  | Consensus {
      participants : List<Agent>,
      method       : ConsensusMethod,
      original_beliefs : List<BeliefId>
    }
\end{lstlisting}

\subsection{The Agent Graph}

Multi-agent beliefs form a graph of contributions:

\begin{center}
\begin{tikzpicture}[
  node distance=2cm,
  belief/.style={rectangle, draw, rounded corners, minimum width=2.5cm, minimum height=1cm},
  agent/.style={rectangle, draw, minimum width=2cm, minimum height=0.8cm},
  ->,>=stealth
]
  \node[belief] (final) {Final Belief\\$c = 0.93$};
  \node[agent, below left=1.5cm and 0.5cm of final] (claude) {Claude\\$c = 0.91$};
  \node[agent, below=1.5cm of final] (human) {Human\\$c = 0.95$};
  \node[agent, below right=1.5cm and 0.5cm of final] (gpt) {GPT\\$c = 0.88$};

  \draw (claude) -- (final);
  \draw (human) -- (final);
  \draw (gpt) -- (final);

  \node[below=0.3cm of final] {Composite Agent};
\end{tikzpicture}
\end{center}

\subsection{Decision Attribution}

Decisions require clear attribution of who made them:

\begin{lstlisting}[language=CLAIR]
decision auth_method : d:auth:001
  question: "How should users authenticate?"
  selected: jwt_hs256

  made_by: AI("claude", "opus-4")

  reviewed_by: [
    (Human("alice"), approved: true, confidence: 0.9),
    (Human("bob"), approved: true, confidence: 0.85)
  ]

  dissenting: [
    (AI("gpt-4"), preferred: session_based, confidence: 0.6)
  ]
\end{lstlisting}

\begin{definition}[Decision Ownership]
\label{def:ownership}
\begin{lstlisting}[language=CLAIR]
type DecisionOwnership =
  | SingleAgent { agent : Agent }
  | SharedOwnership { agents : List<Agent>, weights : List<Confidence> }
  | Delegated { from : Agent, to : Agent, scope : String }
\end{lstlisting}
\end{definition}

\section{Conflict Resolution}
\label{sec:conflict-resolution}

\subsection{Escalation}

When consensus cannot be reached, conflicts escalate:

\begin{lstlisting}[language=CLAIR]
resolve_conflict beliefs =
  if all_agree(beliefs) then
    Resolved(merge(beliefs))
  else
    Escalate {
      conflict: beliefs,
      escalate_to: find_arbiter(beliefs),
      summary: generate_conflict_summary(beliefs)
    }

arbiter_chain = [
  AI("claude", "opus-4"),      -- First try AI resolution
  Human("tech-lead"),           -- Then human tech lead
  Human("team-consensus"),      -- Then team vote
  System("policy-default")      -- Finally, use policy defaults
]
\end{lstlisting}

\subsection{Structured Debate}

Agents can argue for their positions:

\begin{lstlisting}[language=CLAIR]
debate beliefs = {
  positions: group_by_value(beliefs),
  arguments: collect_justifications(beliefs),
  rebuttals: generate_rebuttals(beliefs),
  rounds: iterate_until_stable(argue, max_rounds: 3),
  outcome: final_vote_or_escalate
}
\end{lstlisting}

\subsection{Defer to Specialist}

In domain-specific questions, the most qualified agent takes precedence:

\begin{lstlisting}[language=CLAIR]
resolve_by_expertise beliefs domain =
  let specialist = max_by (\b -> domain_trust(b.agent, domain)) beliefs
  in specialist with justification := specialist_selected(beliefs, domain)
\end{lstlisting}

\section{Anti-Bootstrapping in Collectives}
\label{sec:collective-bootstrapping}

\subsection{The Collective Löb Problem}

Chapter~\ref{ch:self-reference} established that individual agents cannot prove
their own soundness. Does collective agreement escape this limit?

\begin{theorem}[Collective Anti-Bootstrapping]
\label{thm:collective-bootstrap}
A collective of CLAIR agents cannot establish collective infallibility through
mutual agreement. Unanimous consensus does not guarantee truth.
\end{theorem}

\begin{proof}
By Condorcet's theorem, majority voting tracks truth only under independence
and competence. But:
\begin{enumerate}
  \item Independence may fail (correlated training, shared sources)
  \item Competence is assumed, not proven
  \item The collective cannot prove its own competence (Gödelian limit)
\end{enumerate}
Therefore, collective confidence cannot exceed the anti-bootstrapping bound
established for individual agents.
\end{proof}

\begin{corollary}[Collective Fallibilism]
Even unanimous agreement among CLAIR agents should not produce confidence 1.0.
The collective is fallible and should acknowledge this in its confidence assignments.
\end{corollary}

\section{Open Questions}
\label{sec:multi-agent-open}

Several questions remain for future exploration:

\paragraph{Framework Negotiation.}
How do agents with different frameworks reach agreement on which framework to
use? This requires meta-level reasoning about frameworks themselves.

\paragraph{Framework Revision.}
When should the collective revise its shared framework rather than individual
beliefs? This connects to paradigm shifts in philosophy of science.

\paragraph{Strategic Manipulation.}
How should CLAIR detect and handle agents that misrepresent beliefs strategically?
Game-theoretic treatments of multi-agent epistemology apply here.

\paragraph{Meta-Level Trust.}
Should some agents' frameworks be trusted more than others'? How does trust at
the framework level interact with trust at the belief level?

\section{Summary}
\label{sec:multi-agent-summary}

This chapter extended CLAIR to multi-agent settings by:

\begin{enumerate}
  \item \textbf{Philosophical foundation}: Adopting pragmatic internal realism---truth
        is objective within shared frameworks, but there is no view from nowhere

  \item \textbf{Agent attribution}: Extending beliefs with explicit agent attribution
        and supporting nested beliefs about beliefs

  \item \textbf{Framework compatibility}: Requiring compatibility check before
        aggregation, escaping Arrow's impossibility

  \item \textbf{Aggregation mechanisms}: Using $\oplus$ for independent evidence,
        with dependency adjustment for correlated agents

  \item \textbf{Trust dynamics}: Domain-specific trust profiles that evolve based
        on track record

  \item \textbf{Consensus protocols}: Structured protocols for forming consensus
        while preserving minority views

  \item \textbf{Collective anti-bootstrapping}: Establishing that collectives remain
        fallible---unanimous agreement does not guarantee truth
\end{enumerate}

The key insight is that multi-agent CLAIR does not aim for ``the truth'' in a
metaphysically robust sense, but for \emph{convergent approximation within shared
frameworks}. This is both philosophically defensible and practically achievable.

% Chapter 10: Implementation Design
% Reference interpreter architecture for CLAIR

\chapter{Implementation Design}
\label{ch:implementation}

\begin{quote}
\textit{``A language that doesn't affect the way you think about programming is not worth knowing.''}
\begin{flushright}
--- Alan Perlis
\end{flushright}
\end{quote}

\section{From Theory to Practice}

The preceding chapters have developed CLAIR's theoretical foundations: confidence as epistemic commitment, justification as labeled DAGs, safe self-reference via stratification, belief revision extending AGM, and formal verification in Lean. This chapter bridges theory and practice by designing a \emph{reference interpreter}---a minimal, readable implementation that demonstrates CLAIR is not merely a theoretical construct but an implementable language.

A reference interpreter prioritizes clarity over performance. Every evaluation step should correspond directly to the formal semantics. The goal is not to produce industrial-strength tooling but to provide:

\begin{enumerate}
\item \textbf{Proof of concept}: Demonstrating that the theoretical design can be realized
\item \textbf{Testable specification}: A runnable oracle for language semantics
\item \textbf{Foundation for future work}: A basis for optimized implementations
\item \textbf{Validation}: Checking whether theoretical choices create practical difficulties
\end{enumerate}

\section{Language Choice: Haskell vs. Lean}

Two languages merit serious consideration for implementing the reference interpreter: Haskell and Lean 4.

\subsection{Haskell}

Haskell offers:
\begin{itemize}
\item \textbf{Maturity}: Decades of development, extensive libraries, robust tooling
\item \textbf{Expressiveness}: GADTs, type families, and monad transformers cover CLAIR's needs
\item \textbf{Readability}: Haskell code is accessible to researchers outside the theorem proving community
\item \textbf{Performance}: GHC's optimizations provide reasonable execution speed
\item \textbf{Exact arithmetic}: \texttt{Data.Ratio} provides rational numbers matching CLAIR's $[0,1]$ specification
\end{itemize}

\subsection{Lean 4}

Lean 4 offers:
\begin{itemize}
\item \textbf{Verification}: Machine-checked proofs of interpreter correctness
\item \textbf{Extraction}: Code extraction from proofs to executable programs
\item \textbf{Alignment}: Direct connection to the Lean formalization (Chapter~\ref{ch:verification})
\item \textbf{Dependent types}: Rich specifications in the type system
\end{itemize}

\subsection{Recommendation}

We recommend \textbf{Haskell} for the initial reference interpreter.

The primary rationale is \emph{iteration speed}. A Haskell implementation can be written, tested, and refined faster than a verified Lean implementation. The Haskell interpreter serves as a specification that a future Lean implementation can be proven equivalent to.

Additionally, Haskell's accessibility ensures the interpreter can be understood by a broader audience, facilitating adoption and critique.

\section{Core Design Decisions}

\subsection{Evaluation Strategy: Strict}

CLAIR adopts \textbf{strict evaluation} (call-by-value) rather than lazy evaluation.

\textbf{Rationale}: Confidence must be computed at derivation time. Under lazy evaluation, the confidence of an unevaluated thunk is undefined---it depends on what computation would occur if the thunk were forced. This creates conceptual difficulty: what is the epistemic status of a suspended computation?

Strict evaluation ensures that:
\begin{enumerate}
\item Confidence propagation is well-defined at every step
\item Provenance tracking is straightforward (no unevaluated derivations)
\item Performance is predictable
\item Implementation complexity is reduced
\end{enumerate}

The expressiveness cost is minimal. Lazy structures (if needed) can be encoded explicitly using thunks.

\subsection{Confidence Representation: Rationals}

CLAIR represents confidence values as \textbf{rational numbers} (exact fractions).

\textbf{Rationale}: Floating-point arithmetic introduces rounding errors that violate the formal specification. For example, $0.1 + 0.2 \neq 0.3$ in IEEE floating point. Such errors accumulate through derivation chains, potentially causing confidence values to drift outside $[0,1]$ or violating algebraic identities.

Rational arithmetic:
\begin{enumerate}
\item Matches the formal specification (subset of $\mathbb{R}$)
\item Preserves algebraic identities exactly
\item Avoids edge cases in boundedness checks
\item Is well-supported by Haskell's \texttt{Data.Ratio}
\end{enumerate}

The performance cost is acceptable for a reference interpreter. Production implementations might use interval arithmetic or verified floating-point with explicit error bounds.

\subsection{Justification Structure: Hash-Consed DAGs}

Justification graphs are represented as \textbf{hash-consed DAGs with explicit node identifiers}.

\begin{lstlisting}[language=Haskell, caption={Justification graph representation}]
type JustificationId = Int

data JustificationNode
  = JAxiom
  | JRule RuleName [(JustificationId, EdgeType)]
  | JAssumption Assumption
  | JChoice Options Criteria Reason
  | JAbduction JustificationId [JustificationId] Int Reason
  | JAnalogy JustificationId JustificationId TransferPrinciple
  | JInduction [JustificationId] InductiveRule
  | JAggregate [JustificationId] CombinationRule

data EdgeType = Support | Undercut | Rebut

data JustificationGraph = JGraph
  { jgNodes  :: IntMap JustificationNode
  , jgRoot   :: JustificationId
  , jgNextId :: JustificationId
  }
\end{lstlisting}

\textbf{Rationale}: From Chapter~\ref{ch:justification}, justification structures are DAGs, not trees. Shared premises (the same belief used in multiple derivations) require graph structure. Explicit node identifiers enable:

\begin{enumerate}
\item Sharing without reference equality issues
\item Efficient lookup via \texttt{IntMap}
\item Acyclicity checking at construction time
\item Clear correspondence to the formal specification
\end{enumerate}

Hash-consing (storing only unique nodes) reduces memory consumption when premises are widely shared.

\subsection{Error Handling: Typed Errors in Either}

The interpreter uses \textbf{typed errors in an Either monad} rather than exceptions.

\begin{lstlisting}[language=Haskell, caption={Error types}]
data CLAIRError
  = TypeError String
  | ConfidenceOutOfBounds Rational
  | CyclicJustification JustificationId
  | InvalidationTriggered Condition
  | UndefinedVariable String
  | DivisionByZero
  | PatternMatchFailure

type CLAIRResult a = Either CLAIRError a
type CLAIR a = StateT InterpreterState (Either CLAIRError) a
\end{lstlisting}

\textbf{Rationale}: Explicit error handling aligns with CLAIR's epistemic philosophy. Just as beliefs carry metadata about their justification and potential invalidation, computations carry metadata about potential failure modes. There is no hidden control flow; every error path is visible in the types.

\subsection{Invalidation Checking: Lazy with Explicit Trigger}

Invalidation conditions are checked \textbf{lazily}, only when explicitly requested.

\begin{lstlisting}[language=Haskell, caption={Invalidation checking}]
checkValidity :: Belief a -> World -> CLAIR (Maybe InvalidationReason)

useBeliefValue :: Belief a -> CLAIR a
useBeliefValue belief = do
  world <- getCurrentWorld
  validity <- checkValidity belief world
  case validity of
    Nothing     -> pure (beliefValue belief)
    Just reason -> throwError (InvalidationTriggered reason)
\end{lstlisting}

\textbf{Rationale}: Eager invalidation checking (on every belief access) would be prohibitively expensive. Lazy checking with explicit triggers gives programmers control over when to validate beliefs, matching CLAIR's philosophy that the system \emph{tracks} epistemic state rather than \emph{enforcing} particular policies.

\section{Core Types}

\subsection{The Confidence Type}

\begin{lstlisting}[language=Haskell, caption={Confidence type}]
newtype Confidence = Confidence { getConfidence :: Rational }
  deriving (Eq, Ord, Show)

mkConfidence :: Rational -> Either String Confidence
mkConfidence r
  | r < 0     = Left "Confidence cannot be negative"
  | r > 1     = Left "Confidence cannot exceed 1"
  | otherwise = Right (Confidence r)
\end{lstlisting}

The smart constructor \texttt{mkConfidence} enforces the $[0,1]$ invariant at construction time. This is the Haskell analog of the Lean subtype constraint.

\subsection{The Belief Type}

\begin{lstlisting}[language=Haskell, caption={Belief type}]
data Belief a = Belief
  { beliefValue         :: a
  , beliefConfidence    :: Confidence
  , beliefProvenance    :: Provenance
  , beliefJustification :: JustificationGraph
  , beliefInvalidation  :: Set Condition
  } deriving (Show, Functor)
\end{lstlisting}

A belief is a 5-tuple as defined in Chapter~\ref{ch:introduction}. The \texttt{Functor} instance allows mapping over the value while preserving metadata.

\subsection{Provenance}

\begin{lstlisting}[language=Haskell, caption={Provenance tracking}]
data Provenance
  = PLiteral                      -- Hardcoded value
  | PInput String                 -- External input
  | PDerived [ProvenanceRef]      -- Computed from other beliefs
  | PTraining                     -- From LLM training
  | POracle String                -- External authority
  deriving (Show, Eq)
\end{lstlisting}

Provenance tracks where a belief came from, enabling transparency and auditability.

\subsection{Invalidation Conditions}

\begin{lstlisting}[language=Haskell, caption={Invalidation conditions}]
data Condition
  = InputChanged String           -- External input has changed
  | AssumptionFalse String        -- An assumption proved false
  | ConfidenceBelow Confidence    -- Confidence dropped too low
  | ConstraintViolated String     -- A constraint was violated
  | TimeElapsed Integer           -- Too much time has passed (ms)
  deriving (Show, Eq, Ord)
\end{lstlisting}

Invalidation conditions are first-class data, collected into sets and propagated through derivations.

\section{Confidence Operations}

The interpreter implements all confidence operations from Chapters~\ref{ch:confidence} and~\ref{ch:verification}.

\begin{lstlisting}[language=Haskell, caption={Confidence operations}]
-- Multiplication for derivation
mulConf :: Confidence -> Confidence -> Confidence
mulConf (Confidence a) (Confidence b) = Confidence (a * b)

-- Minimum for conservative combination
minConf :: Confidence -> Confidence -> Confidence
minConf (Confidence a) (Confidence b) = Confidence (min a b)

-- Probabilistic OR for aggregation
oplusConf :: Confidence -> Confidence -> Confidence
oplusConf (Confidence a) (Confidence b) = Confidence (a + b - a * b)

-- Undercut: multiplicative discounting
undercutConf :: Confidence -> Confidence -> Confidence
undercutConf (Confidence c) (Confidence d) = Confidence (c * (1 - d))

-- Rebut: probabilistic comparison
rebutConf :: Confidence -> Confidence -> Confidence
rebutConf (Confidence cFor) (Confidence cAgainst)
  | cFor + cAgainst == 0 = Confidence (1 % 2)
  | otherwise = Confidence (cFor / (cFor + cAgainst))
\end{lstlisting}

These implementations match the Lean formalization exactly. The use of rational arithmetic ensures no rounding errors.

\subsection{Aggregation with Combination Rules}

\begin{lstlisting}[language=Haskell, caption={Aggregation}]
data CombinationRule
  = Independent        -- Probabilistic OR
  | Conservative       -- Minimum
  | Multiplicative     -- Product
  | Correlated Rational -- Dependency-adjusted interpolation

aggregateConf :: CombinationRule -> [Confidence] -> Confidence
aggregateConf Independent cs = foldr oplusConf (Confidence 0) cs
aggregateConf Conservative cs = foldr minConf (Confidence 1) cs
aggregateConf Multiplicative cs = foldr mulConf (Confidence 1) cs
aggregateConf (Correlated delta) [c1, c2] =
  let Confidence i = oplusConf c1 c2
      Confidence avg = Confidence ((getConfidence c1 + getConfidence c2) / 2)
  in Confidence ((1 - delta) * i + delta * avg)
\end{lstlisting}

The \texttt{Correlated} combination rule implements the dependency-adjusted aggregation from Chapter~\ref{ch:justification}.

\section{Justification DAG Operations}

\subsection{Acyclicity Checking}

The formal specification requires justification graphs to be acyclic. The interpreter enforces this invariant at edge insertion:

\begin{lstlisting}[language=Haskell, caption={Acyclicity check}]
addJustificationEdge :: JustificationId -> JustificationId -> EdgeType
                     -> JustificationGraph
                     -> Either CLAIRError JustificationGraph
addJustificationEdge from to edgeType graph = do
  if canReach graph to from
    then Left (CyclicJustification from)
    else Right (insertEdge from to edgeType graph)

canReach :: JustificationGraph -> JustificationId -> JustificationId -> Bool
canReach graph src dst = go Set.empty src
  where
    go visited current
      | current == dst = True
      | current `Set.member` visited = False
      | otherwise =
          let visited' = Set.insert current visited
              children = getChildren graph current
          in any (go visited') children
\end{lstlisting}

This ensures the well-foundedness requirement from Chapter~\ref{ch:justification} is maintained.

\subsection{Defeat Evaluation Order}

From Chapter~\ref{ch:justification}, the order of defeat application matters:
\begin{enumerate}
\item First, evaluate all support edges to compute base confidence
\item Then, apply undercuts (weaken inference links)
\item Finally, compare against rebuts (competing conclusions)
\end{enumerate}

\begin{lstlisting}[language=Haskell, caption={Defeat evaluation}]
evaluateConfidenceWithDefeat :: JustificationGraph
                             -> JustificationId
                             -> CLAIR Confidence
evaluateConfidenceWithDefeat graph nodeId = do
  let edges = getEdges graph nodeId
  let (supports, undercuts, rebuts) = partitionEdges edges

  -- Step 1: Base confidence from supports
  supportConfs <- mapM (evaluateConfidenceWithDefeat graph . fst) supports
  let baseConf = aggregateConf Independent supportConfs

  -- Step 2: Apply undercuts
  undercutConfs <- mapM (evaluateConfidenceWithDefeat graph . fst) undercuts
  let combinedUndercut = foldr oplusConf (Confidence 0) undercutConfs
  let afterUndercut = undercutConf baseConf combinedUndercut

  -- Step 3: Compare against rebuts
  rebutConfs <- mapM (evaluateConfidenceWithDefeat graph . fst) rebuts
  let combinedRebut = foldr oplusConf (Confidence 0) rebutConfs
  let finalConf = rebutConf afterUndercut combinedRebut

  pure finalConf
\end{lstlisting}

\subsection{Reinstatement}

A crucial property from Chapter~\ref{ch:justification}: reinstatement emerges compositionally. When a defeater is itself defeated, the original belief regains strength---not through a special mechanism, but through recursive evaluation.

In the code above, when we evaluate undercut confidences, we recursively evaluate their defeaters. A weakened defeater (one that has been undercut) has reduced undercutting power. Thus reinstatement is automatic.

This compositional emergence is a design virtue. It means the system correctly handles arbitrarily nested defeat without special-case logic.

\section{The Evaluator}

\subsection{Runtime Values}

\begin{lstlisting}[language=Haskell, caption={Runtime values}]
data Value
  = VInt Integer
  | VBool Bool
  | VString String
  | VPair Value Value
  | VLeft Value
  | VRight Value
  | VList [Value]
  | VClosure Env String Expr
  | VBelief (Belief Value)
  | VUnit
  deriving (Show)

type Env = Map String Value
\end{lstlisting}

Values include standard lambda calculus constructs plus \texttt{VBelief} for beliefs carrying epistemic metadata.

\subsection{Interpreter State}

\begin{lstlisting}[language=Haskell, caption={Interpreter state}]
data InterpreterState = IState
  { isEnv      :: Env
  , isWorld    :: World
  , isNextProv :: Int
  , isNextJust :: Int
  }

data World = World
  { worldTime        :: Integer
  , worldInputs      :: Map String Value
  , worldAssumptions :: Map String Bool
  }
\end{lstlisting}

The \texttt{World} captures external state relevant for invalidation checking: current time, external inputs, and assumption statuses.

\subsection{Evaluation Function}

The core evaluation function is a recursive descent interpreter:

\begin{lstlisting}[language=Haskell, caption={Core evaluation (excerpt)}]
eval :: Expr -> CLAIR Value
eval expr = case expr of
  EVar x -> do
    env <- gets isEnv
    case Map.lookup x env of
      Just v  -> pure v
      Nothing -> throwError (UndefinedVariable x)

  ELam x _ body -> do
    env <- gets isEnv
    pure (VClosure env x body)

  EApp f arg -> do
    fVal <- eval f
    argVal <- eval arg
    case fVal of
      VClosure env' x body -> withEnv (Map.insert x argVal env') (eval body)
      _ -> throwError (TypeError "Expected function")

  -- Belief creation
  EBelief e -> do
    v <- eval e
    pure $ VBelief $ Belief
      { beliefValue = v
      , beliefConfidence = Confidence 1
      , beliefProvenance = PLiteral
      , beliefJustification = axiomJust
      , beliefInvalidation = mempty
      }

  EBeliefAt e c -> do
    v <- eval e
    conf <- evalConfidence c
    pure $ VBelief $ Belief
      { beliefValue = v
      , beliefConfidence = conf
      , beliefProvenance = PLiteral
      , beliefJustification = axiomJust
      , beliefInvalidation = mempty
      }

  -- Extract value from belief
  EVal e -> do
    v <- eval e
    case v of
      VBelief b -> pure (beliefValue b)
      _ -> throwError (TypeError "Expected belief")

  -- Extract confidence
  EConf e -> do
    v <- eval e
    case v of
      VBelief b -> pure (VConfidence (beliefConfidence b))
      _ -> throwError (TypeError "Expected belief")

  -- Derive new belief
  EDerive beliefs rule combRule -> do
    bVals <- mapM eval beliefs
    bs <- mapM extractBelief bVals
    deriveFromBeliefs rule combRule bs

  -- ... remaining cases for standard constructs
\end{lstlisting}

\subsection{Derivation}

The \texttt{deriveFromBeliefs} function computes new beliefs from premises:

\begin{lstlisting}[language=Haskell, caption={Belief derivation}]
deriveFromBeliefs :: String -> CombinationRule -> [Belief Value] -> CLAIR Value
deriveFromBeliefs ruleName combRule premises = do
  -- Compute new confidence
  let premiseConfs = map beliefConfidence premises
  let newConf = aggregateConf combRule premiseConfs

  -- Build justification graph
  premiseJusts <- mapM (addToJustGraph . beliefJustification) premises
  let newJust = mkRuleJustification ruleName
                  (map (\j -> (j, Support)) premiseJusts)

  -- Combine provenances
  let newProv = PDerived (map (const 0) premises)

  -- Combine invalidation conditions
  let newInv = foldMap beliefInvalidation premises

  -- Apply the derivation rule to compute the value
  newValue <- applyRule ruleName (map beliefValue premises)

  pure $ VBelief $ Belief
    { beliefValue = newValue
    , beliefConfidence = newConf
    , beliefProvenance = newProv
    , beliefJustification = newJust
    , beliefInvalidation = newInv
    }
\end{lstlisting}

Note that invalidation conditions are \emph{accumulated} through derivation. If any premise becomes invalid, the derived belief inherits that invalidity.

\section{Module Structure}

The interpreter is organized into focused modules:

\begin{verbatim}
CLAIR/
├── Types.hs           -- Core types (Confidence, Belief, etc.)
├── Syntax.hs          -- AST definition
├── Parser.hs          -- Surface syntax parser (future work)
├── TypeChecker.hs     -- Type checking
├── Confidence.hs      -- Confidence operations
├── Justification.hs   -- Justification DAG operations
├── Provenance.hs      -- Provenance tracking
├── Invalidation.hs    -- Invalidation checking
├── Evaluator.hs       -- Core evaluation
├── Primitives.hs      -- Built-in operations
├── World.hs           -- World state for invalidation
└── Main.hs            -- REPL and file execution
\end{verbatim}

Each module has a single responsibility, facilitating testing and maintenance.

\section{Testing Strategy}

\subsection{Unit Tests}

Each component is tested in isolation:
\begin{itemize}
\item \textbf{Confidence operations}: Verify algebraic properties (boundedness, associativity, identity)
\item \textbf{Justification graphs}: Verify acyclicity checking, edge insertion, traversal
\item \textbf{Evaluation}: Test each expression form with simple inputs
\end{itemize}

\subsection{Property-Based Tests}

QuickCheck properties verify invariants:

\begin{lstlisting}[language=Haskell, caption={Property-based tests}]
-- Confidence operations preserve bounds
prop_confidenceBounded :: Confidence -> Confidence -> Bool
prop_confidenceBounded c1 c2 =
  let result = oplusConf c1 c2
  in getConfidence result >= 0 && getConfidence result <= 1

-- Undercuts compose via oplus
prop_undercutCompose :: Confidence -> Confidence -> Confidence -> Bool
prop_undercutCompose c d1 d2 =
  undercutConf (undercutConf c d1) d2 == undercutConf c (oplusConf d1 d2)

-- Derivation only decreases confidence (for multiplication)
prop_derivationDecreases :: Confidence -> Confidence -> Bool
prop_derivationDecreases c1 c2 =
  let result = mulConf c1 c2
  in result <= c1 && result <= c2
\end{lstlisting}

These properties connect the implementation to the theorems proven in Chapter~\ref{ch:verification}.

\subsection{Integration Tests}

End-to-end tests verify complete derivation chains:
\begin{itemize}
\item Create beliefs, derive new beliefs, verify confidence propagation
\item Test undercut, rebut, and reinstatement scenarios
\item Verify invalidation detection
\end{itemize}

\section{Scope and Limitations}

\subsection{What the Reference Interpreter Includes}

The minimal viable interpreter includes:
\begin{itemize}
\item Core lambda calculus ($\lambda$, application, let, fix)
\item Products and sums with pattern matching
\item Base types (Int, Bool, String)
\item Belief type with value, confidence, provenance, justification
\item Basic belief operations (\texttt{belief}, \texttt{val}, \texttt{conf}, \texttt{derive})
\item Confidence propagation (multiply, min, oplus)
\item Justification DAG tracking
\item Simple invalidation (manual checking)
\end{itemize}

\subsection{What is Excluded Initially}

To maintain focus, the initial implementation excludes:
\begin{itemize}
\item Effect system (requires additional infrastructure)
\item Full decision syntax (complex evaluation semantics)
\item Modules and imports (standard but time-consuming)
\item Refinement types (requires SMT integration)
\item Parser (use Haskell DSL for initial testing)
\end{itemize}

These can be added incrementally once the core is stable.

\subsection{Estimated Size}

The minimal interpreter is estimated at \textbf{1000--1500 lines of Haskell}. This is small enough to be comprehensible as a specification, yet complete enough to demonstrate CLAIR's novel features.

\section{Future Implementation Work}

\subsection{Runtime Representation (Task 7.2)}

For production use, the runtime representation needs optimization:
\begin{itemize}
\item Memory layout for beliefs (avoiding unnecessary boxing)
\item Efficient confidence arithmetic (possibly using fixed-point)
\item Compact justification graphs (exploiting structural sharing)
\end{itemize}

\subsection{Compilation Strategy (Task 7.3)}

CLAIR programs could be compiled to:
\begin{itemize}
\item \textbf{LLVM}: For native performance
\item \textbf{WebAssembly}: For browser and portable execution
\item \textbf{JVM}: For integration with existing ecosystems
\end{itemize}

The key challenge is preserving epistemic metadata through compilation while enabling optimization.

\subsection{Serialization (Task 7.4)}

For persistent beliefs and distributed systems:
\begin{itemize}
\item Belief serialization format (preserving all metadata)
\item Justification graph serialization (handling sharing)
\item Invalidation condition serialization
\end{itemize}

Open question: Should serialized beliefs maintain their identity across deserializations?

\section{Relationship to Lean Formalization}

The Haskell interpreter and Lean formalization (Chapter~\ref{ch:verification}) are complementary:

\begin{itemize}
\item \textbf{Lean proves mathematical properties}: Boundedness, algebraic laws, composition theorems
\item \textbf{Haskell provides executable semantics}: A runnable specification that can be tested empirically
\end{itemize}

Ideally, a future verified implementation would prove that the Lean formalization and Haskell interpreter are equivalent on shared constructs. This would provide both the assurance of verification and the accessibility of a conventional implementation.

The Lean formalization's discovery that Mathlib's \texttt{unitInterval} is an exact match for CLAIR's confidence type suggests that extraction from Lean could eventually produce a verified interpreter directly.

\section{Conclusion}

This chapter has presented the design for a CLAIR reference interpreter. Key decisions---strict evaluation, rational arithmetic, hash-consed DAGs, typed errors, lazy invalidation---are driven by CLAIR's epistemic focus and the goal of clarity.

The interpreter is small (estimated 1000--1500 lines) but complete enough to demonstrate CLAIR's novel contributions: beliefs as first-class values, confidence propagation through derivation, justification DAG construction, and defeat semantics with reinstatement.

While significant work remains for production-quality tooling, this design establishes that CLAIR's theoretical foundations translate to a practical implementation. The path from formal semantics to running code is clear, if not yet fully traveled.
